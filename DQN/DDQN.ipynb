{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQN(tf.keras.Model):\n",
    "    def __init__(self, n_actions, n_features, learning_rate=0.005, reward_decay=0.9, e_greedy=0.9, replace_target_iter=200, memory_size=3000, batch_size=32, e_greedy_increment=None):\n",
    "        super(DoubleDQN, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory = np.zeros((self.memory_size, n_features*2+2))\n",
    "        self._build_net()\n",
    "        self.optimizer = tf.optimizers.RMSprop(self.lr)\n",
    "\n",
    "    def _build_net(self):\n",
    "        self.eval_net = self.create_network()\n",
    "        self.target_net = self.create_network()\n",
    "\n",
    "    def create_network(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(20, activation='relu', input_shape=(self.n_features,)),\n",
    "            tf.keras.layers.Dense(self.n_actions)\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "        # Flatten the action if it's an array\n",
    "        if isinstance(a, np.ndarray):\n",
    "            a = a.flatten()\n",
    "        # Store transition\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.uniform() < self.epsilon:  # choosing action\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            observation = np.array(observation)[np.newaxis, :]\n",
    "            actions_value = self.eval_net.predict(observation)\n",
    "            action = np.argmax(actions_value)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.target_net.set_weights(self.eval_net.get_weights())\n",
    "\n",
    "        sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        q_next = self.target_net.predict(batch_memory[:, -self.n_features:])\n",
    "        q_eval = self.eval_net.predict(batch_memory[:, :self.n_features])\n",
    "\n",
    "        q_target = q_eval.copy()\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        reward = batch_memory[:, self.n_features + 1]\n",
    "\n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "\n",
    "        self.eval_net.train_on_batch(batch_memory[:, :self.n_features], q_target)\n",
    "\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        return random.sample(self.buffer, sample_size)\n",
    "\n",
    "# Create Model\n",
    "def create_model(num_states, num_actions):\n",
    "    inputs = Input(shape=(num_states,))\n",
    "\n",
    "    # Add the first hidden layer with 64 units and ReLU activation\n",
    "    layer1 = Dense(64, activation=\"relu\")(inputs)\n",
    "    # Add dropout to the first hidden layer\n",
    "    dropout1 = Dropout(0.7)(layer1)\n",
    "\n",
    "    # Add the second hidden layer with 238 units and ReLU activation\n",
    "    layer2 = Dense(128, activation=\"relu\")(dropout1)\n",
    "    # Add dropout to the second hidden layer\n",
    "    dropout2 = Dropout(0.7)(layer2)\n",
    "\n",
    "    # Add the output layer with 'num_actions' units and tanh activation\n",
    "    action = Dense(num_actions, activation=\"tanh\")(dropout2)\n",
    "    return Model(inputs=inputs, outputs=action)\n",
    "\n",
    "def smooth_data(data, window_percent):\n",
    "    window_size = int(len(data) * window_percent)\n",
    "    window_size = max(1, window_size)\n",
    "    smoothed = np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "    return window_size, smoothed\n",
    "\n",
    "def plot_live(loss_log, human_score_log, ylabel='Value'):\n",
    "    # Smooth data\n",
    "    window_size_ll, smoothed_data_ll = smooth_data(loss_log, 0.05)\n",
    "    window_size_hs, smoothed_data_hs = smooth_data(human_score_log, 0.05)\n",
    "\n",
    "    # Plotting\n",
    "    clear_output(wait=True)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "    # Loss Log Graph\n",
    "    ax1.plot(loss_log, label='Original Data', linestyle='--', alpha=0.4)\n",
    "    ax1.plot(np.arange(window_size_ll - 1, len(loss_log)), smoothed_data_ll, label='Smoothed Data')\n",
    "    ax1.set_title(\"Loss Log Over Time\")\n",
    "    ax1.set_ylabel(ylabel)\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Human Score Log Graph\n",
    "    ax2.plot(human_score_log, label='Original Data', linestyle='--', alpha=0.4)\n",
    "    ax2.plot(np.arange(window_size_hs - 1, len(human_score_log)), smoothed_data_hs, label='Smoothed Data')\n",
    "    ax2.set_title(\"Human Score Log Over Time\")\n",
    "    ax2.set_ylabel(ylabel)\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_steps_per_episode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     27\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43mmax_steps_per_episode\u001b[49m):\n\u001b[0;32m     30\u001b[0m     frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Action selection\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_steps_per_episode' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Initialize Environment and Model\n",
    "env = gym.make('Pendulum-v1')\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "\n",
    "# Initialize DoubleDQN models\n",
    "model = DoubleDQN(n_actions=num_actions, n_features=num_states)\n",
    "model_target = DoubleDQN(n_actions=num_actions, n_features=num_states)\n",
    "\n",
    "# Replay Buffer and other settings as before\n",
    "\n",
    "# Training loop\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "human_score_log = []\n",
    "loss_log = []\n",
    "\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "\n",
    "        # Action selection\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = model.choose_action(state)\n",
    "        \n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "# Environment step\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        modified_reward, human_score = reward_mod(reward)\n",
    "        human_score_log.append(human_score)\n",
    "        episode_reward += modified_reward\n",
    "\n",
    "        # Store transition and update state\n",
    "        model.store_transition(state, action, modified_reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if frame_count % update_after_actions == 0 and len(buffer.buffer) > batch_size:\n",
    "            indices = np.random.choice(range(len(buffer.buffer)), size=batch_size)\n",
    "            minibatch = [buffer.buffer[i] for i in indices]\n",
    "\n",
    "            state_sample = np.array([x[0] for x in minibatch])\n",
    "            action_sample = np.array([x[1] for x in minibatch])\n",
    "            rewards_sample = np.array([x[2] for x in minibatch])\n",
    "            next_state_sample = np.array([x[3] for x in minibatch])\n",
    "            done_sample = np.array([x[4] for x in minibatch])\n",
    "\n",
    "            future_rewards = model_target.predict(next_state_sample)\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = model(state_sample)\n",
    "                loss = loss_function(updated_q_values, q_values)\n",
    "                loss_log.append(loss)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "\n",
    "        # Check for end of episode\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Running reward update and logging\n",
    "    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "    # Plotting and saving\n",
    "    if episode_count % 500 == 0:\n",
    "        plot_live(loss_log, human_score_log, ylabel='Value')\n",
    "\n",
    "    if episode_count % 500 == 0:\n",
    "        print(f\"Episode {episode_count}: average reward: {running_reward:.2f}\")\n",
    "\n",
    "    if episode_count % 5000 == 0:\n",
    "        model.save_weights(f'save/model_episode_{episode_count}.h5')\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    # Check for convergence/solution\n",
    "    if running_reward > (reward_mod(0)[0] * 200) * 0.98:\n",
    "        print(f\"Solved at episode {episode_count}: average reward: {running_reward:.2f}!\")\n",
    "        model.save_weights('save/DQNSuccess.h5')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
