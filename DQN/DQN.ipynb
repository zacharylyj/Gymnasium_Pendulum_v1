{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zachary\\anaconda3\\envs\\tf_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalise 27.872827142757767\n",
      "log 3.3629009155432357\n",
      "Solved at episode 0: average reward: 0.05!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 162\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolved at episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: average reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m: episode_reward, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning_reward\u001b[39m\u001b[38;5;124m'\u001b[39m: running_reward, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mloss\u001b[49m}\n\u001b[0;32m    163\u001b[0m tensorboard_callback\u001b[38;5;241m.\u001b[39mon_epoch_end(episode_count, logs)\n\u001b[0;32m    164\u001b[0m episode_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import time\n",
    "import gym\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        return random.sample(self.buffer, sample_size)\n",
    "\n",
    "# log scaling for reward\n",
    "def logarithmic_scaling(reward):\n",
    "    if reward > 0:\n",
    "        return np.log(1 + reward)\n",
    "    elif reward < 0:\n",
    "        return -np.log(1 - reward)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def reward_mod(reward):\n",
    "        normalized_reward = (reward / -16.2736044) * 100\n",
    "        \n",
    "        print(f\"normalise {normalized_reward}\")\n",
    "        # Logarithmic Scaling\n",
    "        log_scaled_reward = logarithmic_scaling(normalized_reward)\n",
    "\n",
    "        print(f\"log {log_scaled_reward}\")\n",
    "        # Clipping Rewards\n",
    "        min_clip_value = -1\n",
    "        max_clip_value = 1\n",
    "        clipped_reward = np.clip(log_scaled_reward, min_clip_value, max_clip_value)\n",
    "        \n",
    "        return clipped_reward, (reward / -16.2736044) * 100\n",
    "\n",
    "# Create Model\n",
    "def create_model(num_states, num_actions):\n",
    "    inputs = Input(shape=(num_states,))\n",
    "    layer1 = Dense(32, activation=\"relu\")(inputs)\n",
    "    layer2 = Dense(64, activation=\"relu\")(layer1)\n",
    "    action = Dense(num_actions, activation=\"tanh\")(layer2) # Using tanh for output\n",
    "    return Model(inputs=inputs, outputs=action)\n",
    "\n",
    "# Initialize Environment and Model\n",
    "env = gym.make('Pendulum-v1', render_mode='human')\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "\n",
    "model = create_model(num_states, num_actions)\n",
    "model_target = create_model(num_states, num_actions)\n",
    "\n",
    "# Learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=10000, decay_rate=0.9, staircase=True)\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "loss_function = Huber()\n",
    "\n",
    "# DQN parameters\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "epsilon_random_frames = 50000\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "batch_size = 32\n",
    "max_steps_per_episode = 200\n",
    "update_after_actions = 4\n",
    "update_target_network = 10000\n",
    "max_memory_length = 100000\n",
    "buffer = ReplayBuffer(max_memory_length)\n",
    "\n",
    "# TensorBoard setup\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping_callback = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "# Training loop\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "\n",
    "\n",
    "human_score_log = []\n",
    "\n",
    "while True:\n",
    "    observation, _ = env.reset()\n",
    "    state = np.array(observation, dtype=np.float32)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action_probs = model.predict(state.reshape(1, -1))[0]\n",
    "            action = np.clip(action_probs, env.action_space.low[0], env.action_space.high[0])\n",
    "\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        modified_reward, human_score = reward_mod(reward)\n",
    "        \n",
    "        human_score_log.append(human_score)\n",
    "        episode_reward += modified_reward\n",
    "\n",
    "        next_state = np.array(next_state)\n",
    "\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if frame_count % update_after_actions == 0 and len(buffer.buffer) > batch_size:\n",
    "            indices = np.random.choice(range(len(buffer.buffer)), size=batch_size)\n",
    "            minibatch = [buffer.buffer[i] for i in indices]\n",
    "\n",
    "            state_sample = np.array([x[0] for x in minibatch])\n",
    "            action_sample = np.array([x[1] for x in minibatch])\n",
    "            rewards_sample = np.array([x[2] for x in minibatch])\n",
    "            next_state_sample = np.array([x[3] for x in minibatch])\n",
    "            done_sample = np.array([x[4] for x in minibatch])\n",
    "\n",
    "            future_rewards = model_target.predict(next_state_sample)\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = model(state_sample)\n",
    "                loss = loss_function(updated_q_values, q_values)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "        if running_reward > -200:\n",
    "            print(f\"Solved at episode {episode_count}: average reward: {running_reward:.2f}!\")\n",
    "            break\n",
    "\n",
    "    logs = {'reward': episode_reward, 'running_reward': running_reward, 'loss': loss}\n",
    "    tensorboard_callback.on_epoch_end(episode_count, logs)\n",
    "    episode_count += 1\n",
    "\n",
    "    if episode_count % 10 == 0:\n",
    "        print(f\"Episode {episode_count}: average reward: {running_reward:.2f}\")\n",
    "\n",
    "    # Save Model\n",
    "    if episode_count % 500 == 0:\n",
    "        model.save('save/model_episode_{}.h5'.format(episode_count))\n",
    "\n",
    "    if episode_count % 1000 == 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
