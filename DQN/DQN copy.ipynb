{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'deque' from 'collection' (c:\\Users\\zachary\\anaconda3\\envs\\tf_env\\lib\\site-packages\\collection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Huber\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'deque' from 'collection' (c:\\Users\\zachary\\anaconda3\\envs\\tf_env\\lib\\site-packages\\collection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Re-importing TensorFlow and necessary libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras import Model\n",
    "from collection import deque\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        return random.sample(self.buffer, sample_size)\n",
    "\n",
    "\n",
    "# Reinitializing the environment and model parameters\n",
    "env = gym.make('Pendulum-v1', render_mode='human')\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "\n",
    "def create_model():\n",
    "    inputs = Input(shape=(num_states,))\n",
    "    layer1 = Dense(32, activation=\"relu\")(inputs)\n",
    "    layer2 = Dense(64, activation=\"relu\")(layer1)\n",
    "    action = Dense(num_actions, activation=\"linear\")(layer2)\n",
    "    return Model(inputs=inputs, outputs=action)\n",
    "\n",
    "model = create_model()\n",
    "model_target = create_model()\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=10000, decay_rate=0.9, staircase=True)\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loss_function = Huber()\n",
    "\n",
    "# DQN parameters\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "epsilon_random_frames = 50000\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "batch_size = 32\n",
    "max_steps_per_episode = 200\n",
    "update_after_actions = 4\n",
    "update_target_network = 10000\n",
    "max_memory_length = 100000\n",
    "\n",
    "# Initialize replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "\n",
    "# Main training loop\n",
    "while True:\n",
    "    observation, _ = env.reset()\n",
    "    state = np.array(observation, dtype=np.float32)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action_probs = model.predict(state.reshape(1, -1))[0]\n",
    "            action = np.clip(action_probs, env.action_space.low[0], env.action_space.high[0])\n",
    "\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        state_next, reward, done, _, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "        episode_reward += reward\n",
    "\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "            \n",
    "            action_sample = np.array(action_sample).astype(int)  # Convert to NumPy array and then cast to int\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = model(state_sample)\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        if running_reward > -200:\n",
    "            print(f\"Solved at episode {episode_count}: average reward: {running_reward:.2f}!\")\n",
    "            break\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_count += 1\n",
    "\n",
    "    if episode_count % 10 == 0:\n",
    "        print(f\"Episode {episode_count}: average reward: {running_reward:.2f}\")\n",
    "        env.render() \n",
    "\n",
    "    # model.save('DQN.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting collection\n",
      "  Downloading collection-0.1.6.tar.gz (5.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: collection\n",
      "  Building wheel for collection (setup.py): started\n",
      "  Building wheel for collection (setup.py): finished with status 'done'\n",
      "  Created wheel for collection: filename=collection-0.1.6-py3-none-any.whl size=5098 sha256=64f15095cc9a24cc77dc7e83400d920fb486a77688d28dfb22594588e229f541\n",
      "  Stored in directory: c:\\users\\zachary\\appdata\\local\\pip\\cache\\wheels\\a5\\70\\eb\\1d28795e9384ab3b9be6359bdde9e1652f6e7dab9d26844f70\n",
      "Successfully built collection\n",
      "Installing collected packages: collection\n",
      "Successfully installed collection-0.1.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
