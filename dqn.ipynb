{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Neural Network for DQN\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 48)\n",
    "        self.fc3 = nn.Linear(48, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Outputs Q-values for all actions\n",
    "        \n",
    "# Priority Replay Buffer\n",
    "class PriorityReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience, priority):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        probabilities = np.array(self.priorities) / sum(self.priorities)\n",
    "        indices = np.random.choice(range(len(self.buffer)), batch_size, p=probabilities)\n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "        return experiences\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "    def get_importance_weights(self, b=0.4, epsilon=1e-5):\n",
    "        sampling_probabilities = np.array(self.priorities) / sum(self.priorities)\n",
    "        importance_weights = (1 / (len(self.buffer) * sampling_probabilities + epsilon)) ** b\n",
    "        return importance_weights\n",
    "\n",
    "# Agent\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, batch_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = PriorityReplayBuffer(10000)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = 0.99  # discount factor\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.model = DQNNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Extract the state array if it's inside a tuple\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        # Ensure the state is a numpy array and reshape it to (1x3)\n",
    "        state = np.array(state).reshape(1, -1)\n",
    "\n",
    "        # Convert the state to a tensor\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get Q-values from the network\n",
    "            q_values = self.model(state_tensor)\n",
    "\n",
    "        # Epsilon-greedy policy for exploration-exploitation trade-off\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            # Exploitation: choose the action with the highest Q-value\n",
    "            action = torch.argmax(q_values).item()\n",
    "            # Scale the action from the range [0, action_size] to [-2, 2]\n",
    "            action = 2.0 * (action / (self.action_size - 1)) - 1.0\n",
    "        else:\n",
    "            # Exploration: choose a random action within the range [-2, 2]\n",
    "            action = np.random.uniform(-2.0, 2.0)\n",
    "\n",
    "        # Decay the epsilon value\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "        \n",
    "        return [action]\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "\n",
    "        # Extract the state array if it's inside a tuple\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        # Ensure the state and next_state are numpy arrays\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            state = np.array(state)\n",
    "        if not isinstance(next_state, np.ndarray):\n",
    "            next_state = np.array(next_state)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        action_tensor = torch.FloatTensor([action]).unsqueeze(0)\n",
    "\n",
    "    def update_network(self):\n",
    "        if self.memory.size() < self.batch_size:\n",
    "            return\n",
    "        experiences = self.memory.sample(self.batch_size)\n",
    "        importance_weights = self.memory.get_importance_weights()\n",
    "\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(experiences):\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            expected = self.model(state)[0][action]\n",
    "            loss = (expected - target) ** 2 * importance_weights[i]  # Adjusting loss with importance weights\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "def calculate_priority(experience):\n",
    "    # Implement your priority calculation here\n",
    "    return 1  # Placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Total Reward: -1413.9347359477272\n",
      "Episode 2/1000, Total Reward: -1553.2451194850046\n",
      "Episode 3/1000, Total Reward: -1848.1158795689653\n",
      "Episode 4/1000, Total Reward: -1191.8535815939351\n",
      "Episode 5/1000, Total Reward: -923.0170103866361\n",
      "Episode 6/1000, Total Reward: -1478.782590907086\n",
      "Episode 7/1000, Total Reward: -1680.7193607095646\n",
      "Episode 8/1000, Total Reward: -1543.3256042381183\n",
      "Episode 9/1000, Total Reward: -1529.2908664532779\n",
      "Episode 10/1000, Total Reward: -1466.1125997897989\n",
      "Episode 11/1000, Total Reward: -1475.1712755798885\n",
      "Episode 12/1000, Total Reward: -1517.7993526551234\n",
      "Episode 13/1000, Total Reward: -1168.7559972419558\n",
      "Episode 14/1000, Total Reward: -1330.2246415272555\n",
      "Episode 15/1000, Total Reward: -1294.615569702171\n",
      "Episode 16/1000, Total Reward: -1561.9860420201585\n",
      "Episode 17/1000, Total Reward: -1848.5275189082838\n",
      "Episode 18/1000, Total Reward: -1240.3353716701276\n",
      "Episode 19/1000, Total Reward: -1561.943235354493\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):  \u001b[38;5;66;03m# Maximum steps in an episode\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m     17\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Action needs to be a list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zachu\\.conda\\envs\\pytorch_env\\lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zachu\\.conda\\envs\\pytorch_env\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zachu\\.conda\\envs\\pytorch_env\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zachu\\.conda\\envs\\pytorch_env\\lib\\site-packages\\gymnasium\\envs\\classic_control\\pendulum.py:235\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m gfxdraw\u001b[38;5;241m.\u001b[39mfilled_circle(\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, rod_end[\u001b[38;5;241m0\u001b[39m], rod_end[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mint\u001b[39m(rod_width \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), (\u001b[38;5;241m204\u001b[39m, \u001b[38;5;241m77\u001b[39m, \u001b[38;5;241m77\u001b[39m)\n\u001b[0;32m    232\u001b[0m )\n\u001b[0;32m    234\u001b[0m fname \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mjoin(path\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massets/clockwise.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 235\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_u \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     scale_img \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39msmoothscale(\n\u001b[0;32m    238\u001b[0m         img,\n\u001b[0;32m    239\u001b[0m         (scale \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_u) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, scale \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_u) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m    240\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('Pendulum-v1', g=9.81, render_mode=\"rgb_array\")\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = 30  # Discretize action space into 10 actions\n",
    "\n",
    "    agent = Agent(state_size, action_size, batch_size=64)\n",
    "\n",
    "    num_episodes = 1000  # Number of episodes to train\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(200):  # Maximum steps in an episode\n",
    "            env.render()\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)  # Action needs to be a list\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            agent.update_network()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
