{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zachu\\AppData\\Local\\Temp\\ipykernel_17364\\755999641.py:64: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  state = torch.FloatTensor(state).unsqueeze(0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 114\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_episodes)\u001b[0m\n\u001b[0;32m    112\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    116\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[1;32mIn[6], line 64\u001b[0m, in \u001b[0;36mAgent.select_action\u001b[1;34m(self, state, noise_scale)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, noise_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m):\n\u001b[1;32m---> 64\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;66;03m# Get the action from the network\u001b[39;00m\n\u001b[0;32m     67\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state, torch\u001b[38;5;241m.\u001b[39mFloatTensor([[\u001b[38;5;241m1\u001b[39m]]))\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 0)"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Neural Network for DQN\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        # Assuming action_size is 1 for continuous action space\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 48)\n",
    "        self.fc3 = nn.Linear(48, action_size)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return torch.sum(q_value * action, dim=1)  # Element-wise product and sum for Q-value\n",
    "\n",
    "# Priority Replay Buffer\n",
    "class PriorityReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience, priority):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        probabilities = np.array(self.priorities) / sum(self.priorities)\n",
    "        indices = np.random.choice(range(len(self.buffer)), batch_size, p=probabilities)\n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "        return experiences\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "    def get_importance_weights(self, b=0.4, epsilon=1e-5):\n",
    "        sampling_probabilities = np.array(self.priorities) / sum(self.priorities)\n",
    "        importance_weights = (1 / (len(self.buffer) * sampling_probabilities + epsilon)) ** b\n",
    "        return importance_weights\n",
    "\n",
    "# Agent\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, batch_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = PriorityReplayBuffer(10000)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = 0.99  # discount factor\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.model = DQNNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def select_action(self, state, noise_scale=0.2):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            # Get the action from the network\n",
    "            action = self.model(state, torch.FloatTensor([[1]])).item()\n",
    "        # Add noise for exploration\n",
    "        action += noise_scale * np.random.randn()\n",
    "        # Clip the action to be within the valid range\n",
    "        return np.clip(action, -2.0, 2.0)\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        # Calculate TD error\n",
    "        current_q = self.model(torch.FloatTensor(state).unsqueeze(0))[0][action].item()\n",
    "        next_q = max(self.model(torch.FloatTensor(next_state).unsqueeze(0)).detach().numpy()[0])\n",
    "        td_error = abs(reward + (self.gamma * next_q * (not done)) - current_q)\n",
    "\n",
    "        self.memory.add((state, action, reward, next_state, done), td_error)\n",
    "\n",
    "    def update_network(self):\n",
    "        if self.memory.size() < self.batch_size:\n",
    "            return\n",
    "        experiences = self.memory.sample(self.batch_size)\n",
    "        importance_weights = self.memory.get_importance_weights()\n",
    "\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(experiences):\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            expected = self.model(state)[0][action]\n",
    "            loss = (expected - target) ** 2 * importance_weights[i]  # Adjusting loss with importance weights\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "def calculate_priority(experience):\n",
    "    # Implement your priority calculation here\n",
    "    return 1  # Placeholder\n",
    "\n",
    "# Training the agent\n",
    "def train(num_episodes=1000):\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.shape[0]\n",
    "    agent = Agent(state_size, action_size, 64)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            agent.update_network()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
