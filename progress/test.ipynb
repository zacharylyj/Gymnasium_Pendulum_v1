{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zachu\\AppData\\Local\\Temp\\ipykernel_11708\\621753468.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  s_batch = torch.tensor(s_lst, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Avg_Score:-1013.9, Epsilon:0.90000\n",
      "Epoch:1, Avg_Score:-1692.4, Epsilon:0.81000\n",
      "Epoch:2, Avg_Score:-1814.4, Epsilon:0.72900\n",
      "Epoch:3, Avg_Score:-980.6, Epsilon:0.65610\n",
      "Epoch:4, Avg_Score:-1206.1, Epsilon:0.59049\n",
      "Epoch:5, Avg_Score:-1141.1, Epsilon:0.53144\n",
      "Epoch:6, Avg_Score:-1240.7, Epsilon:0.47830\n",
      "Epoch:7, Avg_Score:-1328.7, Epsilon:0.43047\n",
      "Epoch:8, Avg_Score:-1234.5, Epsilon:0.38742\n",
      "Epoch:9, Avg_Score:-1392.9, Epsilon:0.34868\n",
      "Epoch:10, Avg_Score:-1385.1, Epsilon:0.31381\n",
      "Epoch:11, Avg_Score:-1288.4, Epsilon:0.28243\n",
      "Epoch:12, Avg_Score:-1488.5, Epsilon:0.25419\n",
      "Epoch:13, Avg_Score:-1398.8, Epsilon:0.22877\n",
      "Epoch:14, Avg_Score:-1192.0, Epsilon:0.20589\n",
      "Epoch:15, Avg_Score:-1245.1, Epsilon:0.18530\n",
      "Epoch:16, Avg_Score:-1199.9, Epsilon:0.16677\n",
      "Epoch:17, Avg_Score:-1091.1, Epsilon:0.15009\n",
      "Epoch:18, Avg_Score:-1344.2, Epsilon:0.13509\n",
      "Epoch:19, Avg_Score:-1069.0, Epsilon:0.12158\n",
      "Epoch:20, Avg_Score:-1156.2, Epsilon:0.10942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 150\u001b[0m\n\u001b[0;32m    146\u001b[0m state_prime, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep([real_action])\n\u001b[0;32m    148\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mput((state, action, reward, state_prime, terminated))\n\u001b[1;32m--> 150\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[0;32m    153\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 108\u001b[0m, in \u001b[0;36mDQNAgent.train_agent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m mini_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m    107\u001b[0m s_batch, a_batch, r_batch, s_prime_batch, done_batch \u001b[38;5;241m=\u001b[39m mini_batch\n\u001b[1;32m--> 108\u001b[0m a_batch \u001b[38;5;241m=\u001b[39m \u001b[43ma_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m td_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_target(mini_batch)\n\u001b[0;32m    112\u001b[0m Q_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ(s_batch)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, a_batch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the action transformation function\n",
    "def transform_action_values(n_actions, min_action=-2.0, max_action=2.0):\n",
    "    linear_actions = np.linspace(-1, 1, n_actions)\n",
    "    non_linear_actions = np.sign(linear_actions) * (linear_actions ** 2)\n",
    "    scaled_actions = min_action + (non_linear_actions + 1) * (max_action - min_action) / 2\n",
    "    return scaled_actions\n",
    "\n",
    "# Define the replay buffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        s_batch = torch.tensor(s_lst, dtype=torch.float)\n",
    "        a_batch = torch.tensor(a_lst, dtype=torch.float)\n",
    "        r_batch = torch.tensor(r_lst, dtype=torch.float)\n",
    "        s_prime_batch = torch.tensor(s_prime_lst, dtype=torch.float)\n",
    "        done_batch = torch.tensor(done_mask_lst, dtype=torch.float)\n",
    "\n",
    "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Define the Q-network class\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, q_lr):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc_1 = nn.Linear(state_dim, 64)\n",
    "        self.fc_2 = nn.Linear(64, 32)\n",
    "        self.fc_out = nn.Linear(32, action_dim)\n",
    "        self.lr = q_lr\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = F.leaky_relu(self.fc_1(x))\n",
    "        q = F.leaky_relu(self.fc_2(q))\n",
    "        q = self.fc_out(q)\n",
    "        return q\n",
    "\n",
    "# Define the DQN agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.state_dim = 3\n",
    "        self.action_dim = 11\n",
    "        self.lr = 0.01\n",
    "        self.gamma = 0.995\n",
    "        self.epsilon = 0.9\n",
    "        self.epsilon_decay = 0.9\n",
    "        self.epsilon_min = 0.001\n",
    "        self.buffer_size = 1000000\n",
    "        self.batch_size = 256\n",
    "        self.memory = ReplayBuffer(self.buffer_size)\n",
    "        self.action_list = transform_action_values(self.action_dim, min_action=-2.0, max_action=2.0)\n",
    "\n",
    "        self.Q = QNetwork(self.state_dim, self.action_dim, self.lr)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        random_number = np.random.rand()\n",
    "        if self.epsilon < random_number:\n",
    "            with torch.no_grad():\n",
    "                action = float(torch.argmax(self.Q(torch.FloatTensor(state))).numpy())\n",
    "        else:\n",
    "            action = float(np.random.choice([n for n in range(self.action_dim)]))\n",
    "        real_action = self.action_list[int(action)]\n",
    "\n",
    "        return action, real_action\n",
    "\n",
    "    def calc_target(self, mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        with torch.no_grad():\n",
    "            q_target = self.Q(s_prime).max(1)[0].unsqueeze(1)\n",
    "            target = r + self.gamma * done * q_target\n",
    "        return target\n",
    "\n",
    "    def train_agent(self):\n",
    "        mini_batch = self.memory.sample(self.batch_size)\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = mini_batch\n",
    "        a_batch = a_batch.type(torch.int64)\n",
    "\n",
    "        td_target = self.calc_target(mini_batch)\n",
    "\n",
    "        Q_a = self.Q(s_batch).gather(1, a_batch)\n",
    "        q_loss = F.smooth_l1_loss(Q_a, td_target)\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        q_loss.mean().backward()\n",
    "        self.Q.optimizer.step()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNAgent()\n",
    "    env = gym.make('Pendulum-v1')\n",
    "\n",
    "    EPOCHS = 100\n",
    "    score_list = []\n",
    "\n",
    "    while agent.memory.size() < 4 * agent.batch_size:\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, real_action = agent.choose_action(state)\n",
    "            state_prime, reward, terminated, truncated, _ = env.step([real_action])\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "            agent.memory.put((state, action, reward, state_prime, terminated))\n",
    "            state = state_prime\n",
    "\n",
    "    for EP in range(EPOCHS):\n",
    "        state, info = env.reset()\n",
    "        score, done = 0.0, False\n",
    "\n",
    "        while not done:\n",
    "            action, real_action = agent.choose_action(state)\n",
    "\n",
    "            state_prime, reward, terminated, truncated, _ = env.step([real_action])\n",
    "\n",
    "            agent.memory.put((state, action, reward, state_prime, terminated))\n",
    "\n",
    "            agent.train_agent()\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            state = state_prime\n",
    "\n",
    "        print(\"Epoch:{}, Avg_Score:{:.1f}, Epsilon:{:.5f}\".format(EP, score, agent.epsilon))\n",
    "        score_list.append(score)\n",
    "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "\n",
    "    plt.plot(score_list)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
