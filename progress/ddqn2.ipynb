{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes 1/150, Score: -840.0264873889257, Epsilion: 1\n",
      "Episodes 2/150, Score: -969.2701907868297, Epsilion: 0.9714136469872765\n",
      "Episodes 3/150, Score: -1299.9490696135333, Epsilion: 0.9333202392875508\n",
      "Episodes 4/150, Score: -1243.5339191846317, Epsilion: 0.8967206418865356\n",
      "Episodes 5/150, Score: -1079.6772617410527, Epsilion: 0.8615562759029154\n",
      "Episodes 6/150, Score: -1072.7159075811069, Epsilion: 0.8277708595913239\n",
      "Episodes 7/150, Score: -1240.9728598385511, Epsilion: 0.7953103182615212\n",
      "Episodes 8/150, Score: -767.6548644373321, Epsilion: 0.7641226977300472\n",
      "Episodes 9/150, Score: -1414.405009456972, Epsilion: 0.7341580811658065\n",
      "Episodes 10/150, Score: -1358.0199782275006, Epsilion: 0.7053685091965097\n",
      "Episodes 11/150, Score: -1091.459329520883, Epsilion: 0.6777079031480937\n",
      "Episodes 12/150, Score: -978.2337839043944, Epsilion: 0.6511319912942588\n",
      "Episodes 13/150, Score: -631.7709352494753, Epsilion: 0.6255982379980887\n",
      "Episodes 14/150, Score: -980.153502229937, Epsilion: 0.601065775632339\n",
      "Episodes 15/150, Score: -587.6741997203284, Epsilion: 0.5774953391694323\n",
      "Episodes 16/150, Score: -626.567793426713, Epsilion: 0.5548492033364647\n",
      "Episodes 17/150, Score: -625.3726929918887, Epsilion: 0.5330911222346448\n",
      "Episodes 18/150, Score: -812.5033608380719, Epsilion: 0.512186271326517\n",
      "Episodes 19/150, Score: -862.2292108057151, Epsilion: 0.49210119169812805\n",
      "Episodes 20/150, Score: -636.7421238718044, Epsilion: 0.47280373650690727\n",
      "Episodes 21/150, Score: -872.7762073185494, Epsilion: 0.4542630195295731\n",
      "Episodes 22/150, Score: -571.9321229903572, Epsilion: 0.43644936572769794\n",
      "Episodes 23/150, Score: -630.1002657760761, Epsilion: 0.4193342637518147\n",
      "Episodes 24/150, Score: -740.3587130676439, Epsilion: 0.4028903203080479\n",
      "Episodes 25/150, Score: -526.1047703998295, Epsilion: 0.3870912163142286\n",
      "Episodes 26/150, Score: -751.3009557423824, Epsilion: 0.3719116647753225\n",
      "Episodes 27/150, Score: -747.8102270488695, Epsilion: 0.35732737031074685\n",
      "Episodes 28/150, Score: -444.2900444268772, Epsilion: 0.343314990268801\n",
      "Episodes 29/150, Score: -500.5071965399951, Epsilion: 0.3298520973659717\n",
      "Episodes 30/150, Score: -500.1391170105324, Epsilion: 0.3169171437913117\n",
      "Episodes 31/150, Score: -386.03053460567344, Epsilion: 0.30448942671844975\n",
      "Episodes 32/150, Score: -324.9339636390909, Epsilion: 0.2925490551700217\n",
      "Episodes 33/150, Score: -768.9785104424514, Epsilion: 0.281076918181496\n",
      "Episodes 34/150, Score: -501.9032409150454, Epsilion: 0.2700546542134354\n",
      "Episodes 35/150, Score: -267.64894929716246, Epsilion: 0.25946462176323676\n",
      "Episodes 36/150, Score: -440.9600239299911, Epsilion: 0.24928987112931694\n",
      "Episodes 37/150, Score: -617.4007421981245, Epsilion: 0.2395141172825466\n",
      "Episodes 38/150, Score: -406.28082890405597, Epsilion: 0.23012171380151586\n",
      "Episodes 39/150, Score: -508.82987136055414, Epsilion: 0.22109762782991382\n",
      "Episodes 40/150, Score: -343.0969990112206, Epsilion: 0.2124274160159373\n",
      "Episodes 41/150, Score: -390.48560939022735, Epsilion: 0.20409720139522358\n",
      "Episodes 42/150, Score: -256.112090281271, Epsilion: 0.19609365118030345\n",
      "Episodes 43/150, Score: -365.4152159045586, Epsilion: 0.18840395542102933\n",
      "Episodes 44/150, Score: -436.175136775939, Epsilion: 0.18101580650182014\n",
      "Episodes 45/150, Score: -388.45275186548565, Epsilion: 0.17391737944291072\n",
      "Episodes 46/150, Score: -254.91312890569134, Epsilion: 0.16709731297407565\n",
      "Episodes 47/150, Score: -261.3464949400407, Epsilion: 0.16054469135053648\n",
      "Episodes 48/150, Score: -259.0963888648924, Epsilion: 0.15424902688194503\n",
      "Episodes 49/150, Score: -280.4660448890255, Epsilion: 0.1482002431464857\n",
      "Episodes 50/150, Score: -263.37099356999204, Epsilion: 0.14238865886322347\n",
      "Episodes 51/150, Score: -131.46650714836483, Epsilion: 0.1368049723968907\n",
      "Episodes 52/150, Score: -2.2224013740640975, Epsilion: 0.131440246870307\n",
      "Episodes 53/150, Score: -383.4836037261357, Epsilion: 0.12628589586060932\n",
      "Episodes 54/150, Score: -126.67995025736353, Epsilion: 0.12133366965639344\n",
      "Episodes 55/150, Score: -263.15201285587324, Epsilion: 0.11657564205377598\n",
      "Episodes 56/150, Score: -267.11792278018055, Epsilion: 0.11200419767023831\n",
      "Episodes 57/150, Score: -386.0581197189067, Epsilion: 0.10761201975595278\n",
      "Episodes 58/150, Score: -262.95991742363776, Epsilion: 0.10339207848307906\n",
      "Episodes 59/150, Score: -260.57216195731246, Epsilion: 0.09933761969428918\n",
      "Episodes 60/150, Score: -573.4290633712607, Epsilion: 0.09544215409251307\n",
      "Episodes 61/150, Score: -136.22499785882698, Epsilion: 0.09169944685460063\n",
      "Episodes 62/150, Score: -258.6573206180203, Epsilion: 0.08810350765227914\n",
      "Episodes 63/150, Score: -258.2619814809827, Epsilion: 0.08464858106443182\n",
      "Episodes 64/150, Score: -130.00754159510532, Epsilion: 0.08132913736535345\n",
      "Episodes 65/150, Score: -384.87490314060386, Epsilion: 0.07813986367423967\n",
      "Episodes 66/150, Score: -256.37524565339254, Epsilion: 0.07507565545174323\n",
      "Episodes 67/150, Score: -250.5716280271175, Epsilion: 0.07213160832998734\n",
      "Episodes 68/150, Score: -260.1673961141766, Epsilion: 0.06930301026296105\n",
      "Episodes 69/150, Score: -378.77523977849717, Epsilion: 0.066585333984732\n",
      "Episodes 70/150, Score: -251.25355970747052, Epsilion: 0.06397422976340528\n",
      "Episodes 71/150, Score: -138.44309984509073, Epsilion: 0.06146551843923211\n",
      "Episodes 72/150, Score: -266.9010476558682, Epsilion: 0.0590551847357245\n",
      "Episodes 73/150, Score: -260.3625019008537, Epsilion: 0.05673937083307091\n",
      "Episodes 74/150, Score: -261.02494272197555, Epsilion: 0.054514370193566454\n",
      "Episodes 75/150, Score: -293.1756678273788, Epsilion: 0.05237662162917507\n",
      "Episodes 76/150, Score: -257.8590934720917, Epsilion: 0.05032270360172887\n",
      "Episodes 77/150, Score: -129.23948155493994, Epsilion: 0.04834932874664181\n",
      "Episodes 78/150, Score: -127.6784591343219, Epsilion: 0.04645333861137252\n",
      "Episodes 79/150, Score: -129.72476980375146, Epsilion: 0.04463169860021507\n",
      "Episodes 80/150, Score: -385.00167790931994, Epsilion: 0.04288149311732723\n",
      "Episodes 81/150, Score: -127.51325271395162, Epsilion: 0.041199920900221336\n",
      "Episodes 82/150, Score: -332.559335973076, Epsilion: 0.03958429053625025\n",
      "Episodes 83/150, Score: -248.91228654875073, Epsilion: 0.0380320161549109\n",
      "Episodes 84/150, Score: -251.8960660232309, Epsilion: 0.036540613289072314\n",
      "Episodes 85/150, Score: -134.55180944753758, Epsilion: 0.03510769489850241\n",
      "Episodes 86/150, Score: -299.5494552928729, Epsilion: 0.033730967549330505\n",
      "Episodes 87/150, Score: -244.68518765576133, Epsilion: 0.03240822774332935\n",
      "Episodes 88/150, Score: -132.62281153294168, Epsilion: 0.0311373583911425\n",
      "Episodes 89/150, Score: -132.55011958372103, Epsilion: 0.029916325423811976\n",
      "Episodes 90/150, Score: -261.1344165677629, Epsilion: 0.028743174537182704\n",
      "Episodes 91/150, Score: -145.9188597857964, Epsilion: 0.02761602806397323\n",
      "Episodes 92/150, Score: -265.1065703305059, Epsilion: 0.02653308196850649\n",
      "Episodes 93/150, Score: -139.75939038396547, Epsilion: 0.025492602959290157\n",
      "Episodes 94/150, Score: -142.270031767196, Epsilion: 0.02449292571482566\n",
      "Episodes 95/150, Score: -257.74086345415, Epsilion: 0.02353245021820526\n",
      "Episodes 96/150, Score: -16.830848923008098, Epsilion: 0.022609639196231504\n",
      "Episodes 97/150, Score: -381.09937135366266, Epsilion: 0.021723015658959947\n",
      "Episodes 98/150, Score: -135.2148882589726, Epsilion: 0.020871160535727247\n",
      "Episodes 99/150, Score: -336.1582421353858, Epsilion: 0.020052710403881117\n",
      "Episodes 100/150, Score: -124.62753848029699, Epsilion: 0.019266355306576635\n",
      "Episodes 101/150, Score: -381.8338600761849, Epsilion: 0.018510836656146556\n",
      "Episodes 102/150, Score: -124.22966623641868, Epsilion: 0.017784945219689506\n",
      "Episodes 103/150, Score: -137.10143492901264, Epsilion: 0.01708751918365216\n",
      "Episodes 104/150, Score: -131.1779560944807, Epsilion: 0.01641744229430796\n",
      "Episodes 105/150, Score: -251.80028084352324, Epsilion: 0.01577364207115545\n",
      "Episodes 106/150, Score: -9.378170654192953, Epsilion: 0.015155088090377442\n",
      "Episodes 107/150, Score: -126.7819754354308, Epsilion: 0.014560790335613076\n",
      "Episodes 108/150, Score: -268.9703868374939, Epsilion: 0.013989797613403566\n",
      "Episodes 109/150, Score: -16.155817775595263, Epsilion: 0.01344119603077519\n",
      "Episodes 110/150, Score: -340.5421744365032, Epsilion: 0.012914107532522964\n",
      "Episodes 111/150, Score: -265.63345970183957, Epsilion: 0.012407688495853888\n",
      "Episodes 112/150, Score: -137.28485215933685, Epsilion: 0.011921128380140433\n",
      "Episodes 113/150, Score: -479.1311873534552, Epsilion: 0.01145364842962312\n",
      "Episodes 114/150, Score: -133.2241677685807, Epsilion: 0.011004500426985827\n",
      "Episodes 115/150, Score: -130.22402342038527, Epsilion: 0.010572965495808904\n",
      "Episodes 116/150, Score: -137.76289850451394, Epsilion: 0.010158352949983452\n",
      "Episodes 117/150, Score: -138.3621439176142, Epsilion: 0.00975999918824503\n",
      "Episodes 118/150, Score: -12.086476956071888, Epsilion: 0.009377266632057592\n",
      "Episodes 119/150, Score: -258.5308194095899, Epsilion: 0.009009542705147717\n",
      "Episodes 120/150, Score: -250.61966254753946, Epsilion: 0.008656238853055782\n",
      "Episodes 121/150, Score: -264.2313139618246, Epsilion: 0.008316789601134817\n",
      "Episodes 122/150, Score: -127.60513882909468, Epsilion: 0.007990651649489363\n",
      "Episodes 123/150, Score: -259.47959593116667, Epsilion: 0.007677303003405883\n",
      "Episodes 124/150, Score: -2.638410012776296, Epsilion: 0.007376242137882652\n",
      "Episodes 125/150, Score: -124.32219187277487, Epsilion: 0.007086987194922258\n",
      "Episodes 126/150, Score: -235.23325502477877, Epsilion: 0.006809075212301692\n",
      "Episodes 127/150, Score: -246.86054994646568, Epsilion: 0.0065420613825858545\n",
      "Episodes 128/150, Score: -129.77669863209513, Epsilion: 0.006285518341198326\n",
      "Episodes 129/150, Score: -137.54105328690193, Epsilion: 0.006039035482410052\n",
      "Episodes 130/150, Score: -134.57419727158685, Epsilion: 0.005802218302151136\n",
      "Episodes 131/150, Score: -122.72746568615598, Epsilion: 0.005574687766593871\n",
      "Episodes 132/150, Score: -243.78607775087053, Epsilion: 0.005356079705496375\n",
      "Episodes 133/150, Score: -126.51452798758643, Epsilion: 0.005146044229335959\n",
      "Episodes 134/150, Score: -134.08531372376612, Epsilion: 0.004944245169299197\n",
      "Episodes 135/150, Score: -265.0297451543712, Epsilion: 0.004750359539232501\n",
      "Episodes 136/150, Score: -126.96955316870071, Epsilion: 0.004564077018691965\n",
      "Episodes 137/150, Score: -258.99538342118876, Epsilion: 0.00438509945626509\n",
      "Episodes 138/150, Score: -135.99176045871263, Epsilion: 0.004213140392369486\n",
      "Episodes 139/150, Score: -259.4889179940816, Epsilion: 0.004047924600764685\n",
      "Episodes 140/150, Score: -265.76486615924347, Epsilion: 0.003889187648043352\n",
      "Episodes 141/150, Score: -246.3323969350206, Epsilion: 0.003736675470396754\n",
      "Episodes 142/150, Score: -138.09668957368143, Epsilion: 0.003590143966977132\n",
      "Episodes 143/150, Score: -127.90473097427912, Epsilion: 0.0034493586092060975\n",
      "Episodes 144/150, Score: -132.69583902503203, Epsilion: 0.0033140940654038157\n",
      "Episodes 145/150, Score: -137.22107146884076, Epsilion: 0.003184133840138086\n",
      "Episodes 146/150, Score: -260.57904047489956, Epsilion: 0.003059269927716171\n",
      "Episodes 147/150, Score: -130.4699618284557, Epsilion: 0.002939302479264697\n",
      "Episodes 148/150, Score: -255.99102000063826, Epsilion: 0.002824039482864866\n",
      "Episodes 149/150, Score: -137.33022260423337, Epsilion: 0.0027132964562309177\n",
      "Episodes 150/150, Score: -131.22976452114668, Epsilion: 0.002606896151440083\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def transform_action_values(n_actions, min_action=-2.0, max_action=2.0):\n",
    "    linear_actions = np.linspace(-1, 1, n_actions)\n",
    "    non_linear_actions = np.sign(linear_actions) * (linear_actions ** 2)\n",
    "    scaled_actions = min_action + (non_linear_actions + 1) * (max_action - min_action) / 2\n",
    "    return scaled_actions\n",
    "\n",
    "# Define the Replay Buffer class\n",
    "class PriorityReplayBuffer:\n",
    "    def __init__(self, buffer_limit, alpha):\n",
    "        self.buffer = []\n",
    "        self.priorities = []\n",
    "        self.buffer_limit = buffer_limit\n",
    "        self.alpha = alpha\n",
    "        self.pos = 0\n",
    "        self.priorities_sum = 0\n",
    "\n",
    "    def put(self, transition, priority):\n",
    "        if len(self.buffer) < self.buffer_limit:\n",
    "            self.buffer.append(transition)\n",
    "            self.priorities.append(priority)\n",
    "        else:\n",
    "            self.buffer[self.pos] = transition\n",
    "            self.priorities_sum -= self.priorities[self.pos]\n",
    "            self.priorities[self.pos] = priority\n",
    "        self.priorities_sum += priority\n",
    "        self.pos = (self.pos + 1) % self.buffer_limit\n",
    "\n",
    "    def sample(self, n, beta):\n",
    "        scaled_priorities = [p ** self.alpha for p in self.priorities]\n",
    "        sample_probs = [p / self.priorities_sum for p in scaled_priorities]\n",
    "        sampled_indices = np.random.choice(len(self.buffer), n, p=sample_probs)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in sampled_indices])\n",
    "        weights = [(1 / (len(self.buffer) * sample_probs[i])) ** beta for i in sampled_indices]\n",
    "\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones), np.array(weights), sampled_indices\n",
    "\n",
    "    def update_priorities(self, indices, new_priorities):\n",
    "        for idx, p in zip(indices, new_priorities):\n",
    "            self.priorities_sum -= self.priorities[idx]\n",
    "            self.priorities[idx] = p\n",
    "            self.priorities_sum += p\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Define the Q-Network class\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.leaky_relu(self.fc1(state))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = PriorityReplayBuffer(buffer_limit=1000000, alpha=0.6)\n",
    "        self.batch_size = 256\n",
    "        self.tau = 0.01\n",
    "        self.gamma = 0.93\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.9998\n",
    "        self.epsilon_min = 0.0001\n",
    "        self.beta_start = 0.4\n",
    "        self.beta_end = 1.0\n",
    "        self.beta_increment_per_sampling = 0.001\n",
    "        self.beta = self.beta_start\n",
    "        self.learning_rate = 0.005\n",
    "        self.model = DQNNetwork(state_size, action_size).to(device)\n",
    "        self.target_model = DQNNetwork(state_size, action_size).to(device)\n",
    "        self.update_target_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.action_values = transform_action_values(action_size)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def select_action(self, state_tensor):\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            action_index = torch.argmax(self.model(state_tensor)).item()\n",
    "            action = self.action_values[action_index]\n",
    "        else:\n",
    "            action_index = np.random.randint(len(self.action_values))\n",
    "            action = self.action_values[action_index]\n",
    "        return action, action_index\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        max_priority = max(self.memory.priorities, default=1.0)\n",
    "        # Store the experience with its priority\n",
    "        self.memory.put((state, action, reward, next_state, done), max_priority)\n",
    "\n",
    "    def train_agent(self):\n",
    "        if self.memory.size() < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.beta = min(self.beta + self.beta_increment_per_sampling, self.beta_end)\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch, weights, indices = self.memory.sample(self.batch_size, self.beta)\n",
    "\n",
    "        # Convert numpy arrays to PyTorch tensors and move them to the device\n",
    "        s_batch = torch.tensor(s_batch, dtype=torch.float32).to(device)\n",
    "        # Ensure a_batch is of dtype torch.int64 for use as an index\n",
    "        a_batch = torch.tensor(a_batch, dtype=torch.int64).to(device).unsqueeze(1)\n",
    "        r_batch = torch.tensor(r_batch, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "        s_prime_batch = torch.tensor(s_prime_batch, dtype=torch.float32).to(device)\n",
    "        done_batch = torch.tensor(done_batch, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "        weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Calculate Q values and expected Q values\n",
    "        q_values = self.model(s_batch).gather(1, a_batch)\n",
    "        next_q_values = self.target_model(s_prime_batch).detach().max(1)[0].unsqueeze(1)\n",
    "        expected_q_values = r_batch + self.gamma * next_q_values * (1 - done_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.smooth_l1_loss(q_values, expected_q_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Epsilon decay\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "        # Soft update the target network\n",
    "        for target_param, param in zip(self.target_model.parameters(), self.model.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "\n",
    "# Main training loop\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = 27\n",
    "\n",
    "    agent = Agent(state_size, action_size)\n",
    "\n",
    "    EPOCHS = 150\n",
    "    score_list = []\n",
    "\n",
    "    for EP in range(EPOCHS):\n",
    "        state, info = env.reset()\n",
    "        score = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Convert the state to a tensor and move it to the correct device\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            # Select an action based on the current state\n",
    "            action, action_index = agent.select_action(state_tensor)\n",
    "\n",
    "            # Perform the action in the environment\n",
    "            next_state, reward, done, truncated, _ = env.step([action])\n",
    "            if done or truncated:\n",
    "                done = True\n",
    "            # Store the experience in the replay buffer\n",
    "            agent.store_experience(state, action_index, reward, next_state, done)\n",
    "            # Train the agent\n",
    "            agent.train_agent()\n",
    "\n",
    "            # Update the state and accumulate the score\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        # Append the score of the EP to the list for plotting\n",
    "        score_list.append(score)\n",
    "        print(f\"Episodes {EP + 1}/{EPOCHS}, Score: {score}, Epsilion: {agent.epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_model(agent, filename=\"prb_dqn_model.pth\"):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': agent.model.state_dict(),\n",
    "        'target_model_state_dict': agent.target_model.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "        'replay_buffer_experiences': agent.memory.buffer,\n",
    "        'replay_buffer_priorities': agent.memory.priorities,\n",
    "        'epsilon': agent.epsilon,\n",
    "        'beta': agent.beta\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "def load_model(agent, filename=\"prb_dqn_model.pth\"):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "        agent.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        agent.target_model.load_state_dict(checkpoint['target_model_state_dict'])\n",
    "        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        agent.memory.buffer = checkpoint['replay_buffer_experiences']\n",
    "        agent.memory.priorities = checkpoint['replay_buffer_priorities'] \n",
    "        agent.epsilon = checkpoint['epsilon']\n",
    "        agent.beta = checkpoint['beta'] \n",
    "    else:\n",
    "        print(\"No saved model found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "save_model(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score over 100 episodes: -195.80360977267833\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate_agent(env, agent, n_episodes=10):\n",
    "    scores = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()  # Assuming env.reset() returns only the state\n",
    "        score = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            action, _ = agent.select_action(state_tensor)  # Exploit the learned policy\n",
    "            next_state, reward, done, truncated, _ = env.step([action])\n",
    "            done = done or truncated\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Load the model and evaluate\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = 27\n",
    "\n",
    "    agent = Agent(state_size, action_size)\n",
    "    load_model(agent)\n",
    "\n",
    "    # Evaluate the agent\n",
    "    scores = evaluate_agent(env, agent, n_episodes=100)\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average Score over 100 episodes: {average_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
