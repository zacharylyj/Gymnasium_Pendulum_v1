{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Avg_Score:-1182.9, Epsilon:1.00000\n",
      "Epoch:1, Avg_Score:-1224.2, Epsilon:0.98000\n",
      "Epoch:2, Avg_Score:-955.6, Epsilon:0.96040\n",
      "Epoch:3, Avg_Score:-885.7, Epsilon:0.94119\n",
      "Epoch:4, Avg_Score:-1337.1, Epsilon:0.92237\n",
      "Epoch:5, Avg_Score:-1494.2, Epsilon:0.90392\n",
      "Epoch:6, Avg_Score:-899.9, Epsilon:0.88584\n",
      "Epoch:7, Avg_Score:-1000.7, Epsilon:0.86813\n",
      "Epoch:8, Avg_Score:-904.1, Epsilon:0.85076\n",
      "Epoch:9, Avg_Score:-1074.2, Epsilon:0.83375\n",
      "Epoch:10, Avg_Score:-977.1, Epsilon:0.81707\n",
      "Epoch:11, Avg_Score:-872.3, Epsilon:0.80073\n",
      "Epoch:12, Avg_Score:-1012.7, Epsilon:0.78472\n",
      "Epoch:13, Avg_Score:-1208.2, Epsilon:0.76902\n",
      "Epoch:14, Avg_Score:-1206.8, Epsilon:0.75364\n",
      "Epoch:15, Avg_Score:-1023.3, Epsilon:0.73857\n",
      "Epoch:16, Avg_Score:-1058.5, Epsilon:0.72380\n",
      "Epoch:17, Avg_Score:-1159.8, Epsilon:0.70932\n",
      "Epoch:18, Avg_Score:-1073.0, Epsilon:0.69514\n",
      "Epoch:19, Avg_Score:-1232.6, Epsilon:0.68123\n",
      "Epoch:20, Avg_Score:-1196.6, Epsilon:0.66761\n",
      "Epoch:21, Avg_Score:-1259.7, Epsilon:0.65426\n",
      "Epoch:22, Avg_Score:-1358.0, Epsilon:0.64117\n",
      "Epoch:23, Avg_Score:-1262.8, Epsilon:0.62835\n",
      "Epoch:24, Avg_Score:-1237.2, Epsilon:0.61578\n",
      "Epoch:25, Avg_Score:-1302.5, Epsilon:0.60346\n",
      "Epoch:26, Avg_Score:-1323.5, Epsilon:0.59140\n",
      "Epoch:27, Avg_Score:-1282.0, Epsilon:0.57957\n",
      "Epoch:28, Avg_Score:-1266.6, Epsilon:0.56798\n",
      "Epoch:29, Avg_Score:-1163.0, Epsilon:0.55662\n",
      "Epoch:30, Avg_Score:-1171.8, Epsilon:0.54548\n",
      "Epoch:31, Avg_Score:-1305.7, Epsilon:0.53457\n",
      "Epoch:32, Avg_Score:-1236.1, Epsilon:0.52388\n",
      "Epoch:33, Avg_Score:-1199.8, Epsilon:0.51341\n",
      "Epoch:34, Avg_Score:-1360.3, Epsilon:0.50314\n",
      "Epoch:35, Avg_Score:-1349.0, Epsilon:0.49307\n",
      "Epoch:36, Avg_Score:-1454.3, Epsilon:0.48321\n",
      "Epoch:37, Avg_Score:-1394.2, Epsilon:0.47355\n",
      "Epoch:38, Avg_Score:-1348.8, Epsilon:0.46408\n",
      "Epoch:39, Avg_Score:-1392.2, Epsilon:0.45480\n",
      "Epoch:40, Avg_Score:-1350.9, Epsilon:0.44570\n",
      "Epoch:41, Avg_Score:-1450.9, Epsilon:0.43679\n",
      "Epoch:42, Avg_Score:-1356.7, Epsilon:0.42805\n",
      "Epoch:43, Avg_Score:-1356.1, Epsilon:0.41949\n",
      "Epoch:44, Avg_Score:-1348.8, Epsilon:0.41110\n",
      "Epoch:45, Avg_Score:-1396.4, Epsilon:0.40288\n",
      "Epoch:46, Avg_Score:-1461.9, Epsilon:0.39482\n",
      "Epoch:47, Avg_Score:-1250.8, Epsilon:0.38692\n",
      "Epoch:48, Avg_Score:-1252.2, Epsilon:0.37919\n",
      "Epoch:49, Avg_Score:-1317.1, Epsilon:0.37160\n",
      "Epoch:50, Avg_Score:-1491.8, Epsilon:0.36417\n",
      "Epoch:51, Avg_Score:-1437.2, Epsilon:0.35689\n",
      "Epoch:52, Avg_Score:-1365.5, Epsilon:0.34975\n",
      "Epoch:53, Avg_Score:-1434.1, Epsilon:0.34275\n",
      "Epoch:54, Avg_Score:-1420.5, Epsilon:0.33590\n",
      "Epoch:55, Avg_Score:-1375.0, Epsilon:0.32918\n",
      "Epoch:56, Avg_Score:-1302.9, Epsilon:0.32260\n",
      "Epoch:57, Avg_Score:-1368.4, Epsilon:0.31614\n",
      "Epoch:58, Avg_Score:-1360.1, Epsilon:0.30982\n",
      "Epoch:59, Avg_Score:-1369.9, Epsilon:0.30363\n",
      "Epoch:60, Avg_Score:-1442.3, Epsilon:0.29755\n",
      "Epoch:61, Avg_Score:-1474.7, Epsilon:0.29160\n",
      "Epoch:62, Avg_Score:-1492.8, Epsilon:0.28577\n",
      "Epoch:63, Avg_Score:-1460.2, Epsilon:0.28005\n",
      "Epoch:64, Avg_Score:-1521.5, Epsilon:0.27445\n",
      "Epoch:65, Avg_Score:-1385.8, Epsilon:0.26896\n",
      "Epoch:66, Avg_Score:-1336.2, Epsilon:0.26359\n",
      "Epoch:67, Avg_Score:-1524.2, Epsilon:0.25831\n",
      "Epoch:68, Avg_Score:-1501.3, Epsilon:0.25315\n",
      "Epoch:69, Avg_Score:-1400.6, Epsilon:0.24808\n",
      "Epoch:70, Avg_Score:-1506.8, Epsilon:0.24312\n",
      "Epoch:71, Avg_Score:-1499.7, Epsilon:0.23826\n",
      "Epoch:72, Avg_Score:-1398.3, Epsilon:0.23349\n",
      "Epoch:73, Avg_Score:-1521.6, Epsilon:0.22883\n",
      "Epoch:74, Avg_Score:-1532.7, Epsilon:0.22425\n",
      "Epoch:75, Avg_Score:-1512.1, Epsilon:0.21976\n",
      "Epoch:76, Avg_Score:-1541.8, Epsilon:0.21537\n",
      "Epoch:77, Avg_Score:-1453.2, Epsilon:0.21106\n",
      "Epoch:78, Avg_Score:-1326.9, Epsilon:0.20684\n",
      "Epoch:79, Avg_Score:-1537.1, Epsilon:0.20270\n",
      "Epoch:80, Avg_Score:-1533.2, Epsilon:0.19865\n",
      "Epoch:81, Avg_Score:-1505.4, Epsilon:0.19468\n",
      "Epoch:82, Avg_Score:-1516.7, Epsilon:0.19078\n",
      "Epoch:83, Avg_Score:-1565.1, Epsilon:0.18697\n",
      "Epoch:84, Avg_Score:-1516.6, Epsilon:0.18323\n",
      "Epoch:85, Avg_Score:-1538.0, Epsilon:0.17956\n",
      "Epoch:86, Avg_Score:-1434.5, Epsilon:0.17597\n",
      "Epoch:87, Avg_Score:-1518.0, Epsilon:0.17245\n",
      "Epoch:88, Avg_Score:-1418.6, Epsilon:0.16900\n",
      "Epoch:89, Avg_Score:-1517.9, Epsilon:0.16562\n",
      "Epoch:90, Avg_Score:-1562.8, Epsilon:0.16231\n",
      "Epoch:91, Avg_Score:-1520.6, Epsilon:0.15906\n",
      "Epoch:92, Avg_Score:-1468.4, Epsilon:0.15588\n",
      "Epoch:93, Avg_Score:-1540.9, Epsilon:0.15277\n",
      "Epoch:94, Avg_Score:-1559.5, Epsilon:0.14971\n",
      "Epoch:95, Avg_Score:-1468.9, Epsilon:0.14672\n",
      "Epoch:96, Avg_Score:-1465.5, Epsilon:0.14378\n",
      "Epoch:97, Avg_Score:-1479.3, Epsilon:0.14091\n",
      "Epoch:98, Avg_Score:-1496.7, Epsilon:0.13809\n",
      "Epoch:99, Avg_Score:-1494.2, Epsilon:0.13533\n",
      "Epoch:100, Avg_Score:-1620.5, Epsilon:0.13262\n",
      "Epoch:101, Avg_Score:-1485.0, Epsilon:0.12997\n",
      "Epoch:102, Avg_Score:-1538.8, Epsilon:0.12737\n",
      "Epoch:103, Avg_Score:-1541.8, Epsilon:0.12482\n",
      "Epoch:104, Avg_Score:-1549.0, Epsilon:0.12232\n",
      "Epoch:105, Avg_Score:-1529.9, Epsilon:0.11988\n",
      "Epoch:106, Avg_Score:-1573.7, Epsilon:0.11748\n",
      "Epoch:107, Avg_Score:-1461.9, Epsilon:0.11513\n",
      "Epoch:108, Avg_Score:-1536.6, Epsilon:0.11283\n",
      "Epoch:109, Avg_Score:-1546.3, Epsilon:0.11057\n",
      "Epoch:110, Avg_Score:-1530.7, Epsilon:0.10836\n",
      "Epoch:111, Avg_Score:-1521.6, Epsilon:0.10619\n",
      "Epoch:112, Avg_Score:-1598.0, Epsilon:0.10407\n",
      "Epoch:113, Avg_Score:-1524.5, Epsilon:0.10199\n",
      "Epoch:114, Avg_Score:-1477.3, Epsilon:0.09995\n",
      "Epoch:115, Avg_Score:-1554.0, Epsilon:0.09795\n",
      "Epoch:116, Avg_Score:-1566.0, Epsilon:0.09599\n",
      "Epoch:117, Avg_Score:-1540.5, Epsilon:0.09407\n",
      "Epoch:118, Avg_Score:-1542.4, Epsilon:0.09219\n",
      "Epoch:119, Avg_Score:-1563.0, Epsilon:0.09034\n",
      "Epoch:120, Avg_Score:-1520.9, Epsilon:0.08854\n",
      "Epoch:121, Avg_Score:-1562.4, Epsilon:0.08677\n",
      "Epoch:122, Avg_Score:-1547.1, Epsilon:0.08503\n",
      "Epoch:123, Avg_Score:-1438.6, Epsilon:0.08333\n",
      "Epoch:124, Avg_Score:-1538.6, Epsilon:0.08166\n",
      "Epoch:125, Avg_Score:-1455.4, Epsilon:0.08003\n",
      "Epoch:126, Avg_Score:-1496.3, Epsilon:0.07843\n",
      "Epoch:127, Avg_Score:-1537.6, Epsilon:0.07686\n",
      "Epoch:128, Avg_Score:-1595.7, Epsilon:0.07532\n",
      "Epoch:129, Avg_Score:-1625.1, Epsilon:0.07382\n",
      "Epoch:130, Avg_Score:-1461.5, Epsilon:0.07234\n",
      "Epoch:131, Avg_Score:-1566.0, Epsilon:0.07090\n",
      "Epoch:132, Avg_Score:-1636.9, Epsilon:0.06948\n",
      "Epoch:133, Avg_Score:-1597.7, Epsilon:0.06809\n",
      "Epoch:134, Avg_Score:-1564.4, Epsilon:0.06673\n",
      "Epoch:135, Avg_Score:-1587.5, Epsilon:0.06539\n",
      "Epoch:136, Avg_Score:-1593.0, Epsilon:0.06408\n",
      "Epoch:137, Avg_Score:-1554.7, Epsilon:0.06280\n",
      "Epoch:138, Avg_Score:-1576.6, Epsilon:0.06155\n",
      "Epoch:139, Avg_Score:-1516.0, Epsilon:0.06031\n",
      "Epoch:140, Avg_Score:-1564.0, Epsilon:0.05911\n",
      "Epoch:141, Avg_Score:-1531.3, Epsilon:0.05793\n",
      "Epoch:142, Avg_Score:-1546.0, Epsilon:0.05677\n",
      "Epoch:143, Avg_Score:-1537.6, Epsilon:0.05563\n",
      "Epoch:144, Avg_Score:-1599.8, Epsilon:0.05452\n",
      "Epoch:145, Avg_Score:-1545.6, Epsilon:0.05343\n",
      "Epoch:146, Avg_Score:-1471.6, Epsilon:0.05236\n",
      "Epoch:147, Avg_Score:-1551.8, Epsilon:0.05131\n",
      "Epoch:148, Avg_Score:-1565.9, Epsilon:0.05029\n",
      "Epoch:149, Avg_Score:-1564.3, Epsilon:0.04928\n",
      "Epoch:150, Avg_Score:-1594.6, Epsilon:0.04830\n",
      "Epoch:151, Avg_Score:-1617.8, Epsilon:0.04733\n",
      "Epoch:152, Avg_Score:-1582.1, Epsilon:0.04638\n",
      "Epoch:153, Avg_Score:-1580.5, Epsilon:0.04546\n",
      "Epoch:154, Avg_Score:-1645.7, Epsilon:0.04455\n",
      "Epoch:155, Avg_Score:-1572.6, Epsilon:0.04366\n",
      "Epoch:156, Avg_Score:-1499.9, Epsilon:0.04278\n",
      "Epoch:157, Avg_Score:-1543.2, Epsilon:0.04193\n",
      "Epoch:158, Avg_Score:-1573.6, Epsilon:0.04109\n",
      "Epoch:159, Avg_Score:-1534.2, Epsilon:0.04027\n",
      "Epoch:160, Avg_Score:-1618.1, Epsilon:0.03946\n",
      "Epoch:161, Avg_Score:-1568.0, Epsilon:0.03867\n",
      "Epoch:162, Avg_Score:-1589.2, Epsilon:0.03790\n",
      "Epoch:163, Avg_Score:-1542.6, Epsilon:0.03714\n",
      "Epoch:164, Avg_Score:-1541.9, Epsilon:0.03640\n",
      "Epoch:165, Avg_Score:-1591.5, Epsilon:0.03567\n",
      "Epoch:166, Avg_Score:-1609.8, Epsilon:0.03496\n",
      "Epoch:167, Avg_Score:-1583.4, Epsilon:0.03426\n",
      "Epoch:168, Avg_Score:-1619.1, Epsilon:0.03357\n",
      "Epoch:169, Avg_Score:-1546.0, Epsilon:0.03290\n",
      "Epoch:170, Avg_Score:-1581.9, Epsilon:0.03224\n",
      "Epoch:171, Avg_Score:-1614.9, Epsilon:0.03160\n",
      "Epoch:172, Avg_Score:-1606.3, Epsilon:0.03097\n",
      "Epoch:173, Avg_Score:-1561.5, Epsilon:0.03035\n",
      "Epoch:174, Avg_Score:-1612.9, Epsilon:0.02974\n",
      "Epoch:175, Avg_Score:-1545.1, Epsilon:0.02914\n",
      "Epoch:176, Avg_Score:-1587.2, Epsilon:0.02856\n",
      "Epoch:177, Avg_Score:-1586.0, Epsilon:0.02799\n",
      "Epoch:178, Avg_Score:-1538.8, Epsilon:0.02743\n",
      "Epoch:179, Avg_Score:-1546.3, Epsilon:0.02688\n",
      "Epoch:180, Avg_Score:-1551.4, Epsilon:0.02634\n",
      "Epoch:181, Avg_Score:-1457.2, Epsilon:0.02582\n",
      "Epoch:182, Avg_Score:-1546.5, Epsilon:0.02530\n",
      "Epoch:183, Avg_Score:-1515.6, Epsilon:0.02480\n",
      "Epoch:184, Avg_Score:-1538.8, Epsilon:0.02430\n",
      "Epoch:185, Avg_Score:-1585.1, Epsilon:0.02381\n",
      "Epoch:186, Avg_Score:-1609.3, Epsilon:0.02334\n",
      "Epoch:187, Avg_Score:-1590.3, Epsilon:0.02287\n",
      "Epoch:188, Avg_Score:-1591.4, Epsilon:0.02241\n",
      "Epoch:189, Avg_Score:-1530.8, Epsilon:0.02196\n",
      "Epoch:190, Avg_Score:-1546.3, Epsilon:0.02153\n",
      "Epoch:191, Avg_Score:-1558.1, Epsilon:0.02110\n",
      "Epoch:192, Avg_Score:-1387.1, Epsilon:0.02067\n",
      "Epoch:193, Avg_Score:-1564.0, Epsilon:0.02026\n",
      "Epoch:194, Avg_Score:-1580.6, Epsilon:0.01985\n",
      "Epoch:195, Avg_Score:-1564.8, Epsilon:0.01946\n",
      "Epoch:196, Avg_Score:-1500.8, Epsilon:0.01907\n",
      "Epoch:197, Avg_Score:-1533.6, Epsilon:0.01869\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the action transformation function\n",
    "def transform_action_values(n_actions, min_action=-2.0, max_action=2.0):\n",
    "    linear_actions = np.linspace(-1, 1, n_actions)\n",
    "    non_linear_actions = np.sign(linear_actions) * (linear_actions ** 2)\n",
    "    scaled_actions = min_action + (non_linear_actions + 1) * (max_action - min_action) / 2\n",
    "    return scaled_actions\n",
    "\n",
    "# Define the replay buffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        s_batch = torch.tensor(s_lst, dtype=torch.float)\n",
    "        a_batch = torch.tensor(a_lst, dtype=torch.float)\n",
    "        r_batch = torch.tensor(r_lst, dtype=torch.float)\n",
    "        s_prime_batch = torch.tensor(s_prime_lst, dtype=torch.float)\n",
    "        done_batch = torch.tensor(done_mask_lst, dtype=torch.float)\n",
    "\n",
    "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Define the Q-network class\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, q_lr):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc_1 = nn.Linear(state_dim, 64)\n",
    "        self.fc_2 = nn.Linear(64, 32)\n",
    "        self.fc_out = nn.Linear(32, action_dim)\n",
    "        self.lr = q_lr\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = F.leaky_relu(self.fc_1(x))\n",
    "        q = F.leaky_relu(self.fc_2(q))\n",
    "        q = self.fc_out(q)\n",
    "        return q\n",
    "\n",
    "# Define the DQN agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, gamma_epoch):\n",
    "        self.state_dim = 3\n",
    "        self.action_dim = 11\n",
    "        self.lr = 0.005\n",
    "        self.gamma = (1-(1/gamma_epoch))\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.98\n",
    "        self.epsilon_min = 0.001\n",
    "        self.buffer_size = 1000000\n",
    "        self.batch_size = 256\n",
    "        self.memory = ReplayBuffer(self.buffer_size)\n",
    "        self.action_list = transform_action_values(self.action_dim, min_action=-2.0, max_action=2.0)\n",
    "        self.Q = QNetwork(self.state_dim, self.action_dim, self.lr)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        random_number = np.random.rand()\n",
    "        if self.epsilon < random_number:\n",
    "            with torch.no_grad():\n",
    "                action = float(torch.argmax(self.Q(torch.FloatTensor(state))).numpy())\n",
    "        else:\n",
    "            action = float(np.random.choice([n for n in range(self.action_dim)]))\n",
    "        real_action = self.action_list[int(action)]\n",
    "\n",
    "        return action, real_action\n",
    "\n",
    "    def calc_target(self, mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        with torch.no_grad():\n",
    "            q_target = self.Q(s_prime).max(1)[0].unsqueeze(1)\n",
    "            target = r + self.gamma * done * q_target\n",
    "        return target\n",
    "\n",
    "    def train_agent(self):\n",
    "        mini_batch = self.memory.sample(self.batch_size)\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = mini_batch\n",
    "        a_batch = a_batch.type(torch.int64)\n",
    "\n",
    "        td_target = self.calc_target(mini_batch)\n",
    "\n",
    "        Q_a = self.Q(s_batch).gather(1, a_batch)\n",
    "        q_loss = F.smooth_l1_loss(Q_a, td_target)\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        q_loss.mean().backward()\n",
    "        self.Q.optimizer.step()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in range(60):\n",
    "        number = (i*0.5)+0.5\n",
    "        if number > 25:\n",
    "            break\n",
    "\n",
    "        agent = DQNAgent(gamma_epoch=number)\n",
    "        env = gym.make('Pendulum-v1')\n",
    "\n",
    "        EPOCHS = 200\n",
    "        score_list = []\n",
    "\n",
    "        while agent.memory.size() < 4 * agent.batch_size:\n",
    "            state, info = env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action, real_action = agent.select_action(state)\n",
    "                state_prime, reward, terminated, truncated, _ = env.step([real_action])\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    done = True\n",
    "\n",
    "                agent.memory.put((state, action, reward, state_prime, terminated))\n",
    "                state = state_prime\n",
    "\n",
    "        for EP in range(EPOCHS):\n",
    "            state, info = env.reset()\n",
    "            score, done = 0.0, False\n",
    "\n",
    "            while not done:\n",
    "                action, real_action = agent.select_action(state)\n",
    "\n",
    "                state_prime, reward, terminated, truncated, _ = env.step([real_action])\n",
    "                agent.memory.put((state, action, reward, state_prime, terminated))\n",
    "\n",
    "                agent.train_agent()\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    done = True\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                state = state_prime\n",
    "\n",
    "            print(\"Epoch:{}, Avg_Score:{:.1f}, Epsilon:{:.5f}\".format(EP, score, agent.epsilon))\n",
    "            score_list.append(score)\n",
    "            agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "\n",
    "        plt.plot(score_list)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'image/gamma-{agent.gamma}.png')  # Specify your path here\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
