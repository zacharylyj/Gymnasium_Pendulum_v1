{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zachary\\AppData\\Local\\Temp\\ipykernel_18760\\300922213.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  s_batch = torch.tensor(s_lst, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Avg_Score:-1061.6, Epsilon:1.00000\n",
      "Epoch:1, Avg_Score:-1188.5, Epsilon:0.98000\n",
      "Epoch:2, Avg_Score:-1001.1, Epsilon:0.96040\n",
      "Epoch:3, Avg_Score:-1425.4, Epsilon:0.94119\n",
      "Epoch:4, Avg_Score:-879.3, Epsilon:0.92237\n",
      "Epoch:5, Avg_Score:-1005.4, Epsilon:0.90392\n",
      "Epoch:6, Avg_Score:-1474.9, Epsilon:0.88584\n",
      "Epoch:7, Avg_Score:-890.1, Epsilon:0.86813\n",
      "Epoch:8, Avg_Score:-915.7, Epsilon:0.85076\n",
      "Epoch:9, Avg_Score:-980.0, Epsilon:0.83375\n",
      "Epoch:10, Avg_Score:-1062.6, Epsilon:0.81707\n",
      "Epoch:11, Avg_Score:-1098.2, Epsilon:0.80073\n",
      "Epoch:12, Avg_Score:-1101.9, Epsilon:0.78472\n",
      "Epoch:13, Avg_Score:-1042.9, Epsilon:0.76902\n",
      "Epoch:14, Avg_Score:-1146.5, Epsilon:0.75364\n",
      "Epoch:15, Avg_Score:-1087.7, Epsilon:0.73857\n",
      "Epoch:16, Avg_Score:-1012.9, Epsilon:0.72380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 150\u001b[0m\n\u001b[0;32m    147\u001b[0m state_prime, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep([real_action])\n\u001b[0;32m    148\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mput((state, action, reward, state_prime, terminated))\n\u001b[1;32m--> 150\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[0;32m    153\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [1], line 110\u001b[0m, in \u001b[0;36mDQNAgent.train_agent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m Q_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ(s_batch)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, a_batch)\n\u001b[0;32m    109\u001b[0m q_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msmooth_l1_loss(Q_a, td_target)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m q_loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\optim\\optimizer.py:803\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    801\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    805\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\autograd\\profiler.py:648\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 648\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\_ops.py:448\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the action transformation function\n",
    "def transform_action_values(n_actions, min_action=-2.0, max_action=2.0):\n",
    "    linear_actions = np.linspace(-1, 1, n_actions)\n",
    "    non_linear_actions = np.sign(linear_actions) * (linear_actions ** 2)\n",
    "    scaled_actions = min_action + (non_linear_actions + 1) * (max_action - min_action) / 2\n",
    "    return scaled_actions\n",
    "\n",
    "# Define the replay buffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        s_batch = torch.tensor(s_lst, dtype=torch.float)\n",
    "        a_batch = torch.tensor(a_lst, dtype=torch.float)\n",
    "        r_batch = torch.tensor(r_lst, dtype=torch.float)\n",
    "        s_prime_batch = torch.tensor(s_prime_lst, dtype=torch.float)\n",
    "        done_batch = torch.tensor(done_mask_lst, dtype=torch.float)\n",
    "\n",
    "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Define the Q-network class\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, q_lr):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc_1 = nn.Linear(state_dim, 64)\n",
    "        self.fc_2 = nn.Linear(64, 32)\n",
    "        self.fc_out = nn.Linear(32, action_dim)\n",
    "        self.lr = q_lr\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = F.leaky_relu(self.fc_1(x))\n",
    "        q = F.leaky_relu(self.fc_2(q))\n",
    "        q = self.fc_out(q)\n",
    "        return q\n",
    "\n",
    "# Define the DQN agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, gamma_epoch):\n",
    "        self.state_dim = 3\n",
    "        self.action_dim = 11\n",
    "        self.lr = 0.005\n",
    "        self.gamma = (1-(1/gamma_epoch))\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.98\n",
    "        self.epsilon_min = 0.001\n",
    "        self.buffer_size = 1000000\n",
    "        self.batch_size = 256\n",
    "        self.memory = ReplayBuffer(self.buffer_size)\n",
    "        self.action_list = transform_action_values(self.action_dim, min_action=-2.0, max_action=2.0)\n",
    "        self.Q = QNetwork(self.state_dim, self.action_dim, self.lr)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        random_number = np.random.rand()\n",
    "        if self.epsilon < random_number:\n",
    "            with torch.no_grad():\n",
    "                action = float(torch.argmax(self.Q(torch.FloatTensor(state))).numpy())\n",
    "        else:\n",
    "            action = float(np.random.choice([n for n in range(self.action_dim)]))\n",
    "        real_action = self.action_list[int(action)]\n",
    "\n",
    "        return action, real_action\n",
    "\n",
    "    def calc_target(self, mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        with torch.no_grad():\n",
    "            q_target = self.Q(s_prime).max(1)[0].unsqueeze(1)\n",
    "            target = r + self.gamma * done * q_target\n",
    "        return target\n",
    "\n",
    "    def train_agent(self):\n",
    "        mini_batch = self.memory.sample(self.batch_size)\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = mini_batch\n",
    "        a_batch = a_batch.type(torch.int64)\n",
    "\n",
    "        td_target = self.calc_target(mini_batch)\n",
    "\n",
    "        Q_a = self.Q(s_batch).gather(1, a_batch)\n",
    "        q_loss = F.smooth_l1_loss(Q_a, td_target)\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        q_loss.mean().backward()\n",
    "        self.Q.optimizer.step()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in range(60):\n",
    "        number = (i*0.5)+0.5\n",
    "        if number > 25:\n",
    "            break\n",
    "\n",
    "        agent = DQNAgent(gamma_epoch=number)\n",
    "        env = gym.make('Pendulum-v1')\n",
    "\n",
    "        EPOCHS = 200\n",
    "        score_list = []\n",
    "\n",
    "        while agent.memory.size() < 4 * agent.batch_size:\n",
    "            state, info = env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action, real_action = agent.select_action(state)\n",
    "                state_prime, reward, terminated, truncated, _ = env.step([real_action])\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    done = True\n",
    "\n",
    "                agent.memory.put((state, action, reward, state_prime, terminated))\n",
    "                state = state_prime\n",
    "\n",
    "        for EP in range(EPOCHS):\n",
    "            state, info = env.reset()\n",
    "            score, done = 0.0, False\n",
    "\n",
    "            while not done:\n",
    "                action, real_action = agent.select_action(state)\n",
    "\n",
    "                state_prime, reward, terminated, truncated, _ = env.step([real_action])\n",
    "                agent.memory.put((state, action, reward, state_prime, terminated))\n",
    "\n",
    "                agent.train_agent()\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    done = True\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                state = state_prime\n",
    "\n",
    "            print(\"Epoch:{}, Avg_Score:{:.1f}, Epsilon:{:.5f}\".format(EP, score, agent.epsilon))\n",
    "            score_list.append(score)\n",
    "            agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "\n",
    "        plt.plot(score_list)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'image/gamma-{agent.gamma}.png')  # Specify your path here\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
