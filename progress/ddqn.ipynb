{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0, Avg_Score:-1366.1, MaxQ_Action_Count:17, Epsilon:0.90000\n",
      "Episode:1, Avg_Score:-1812.2, MaxQ_Action_Count:40, Epsilon:0.81000\n",
      "Episode:2, Avg_Score:-1057.3, MaxQ_Action_Count:55, Epsilon:0.72900\n",
      "Episode:3, Avg_Score:-1198.3, MaxQ_Action_Count:70, Epsilon:0.65610\n",
      "Episode:4, Avg_Score:-1350.8, MaxQ_Action_Count:68, Epsilon:0.59049\n",
      "Episode:5, Avg_Score:-983.5, MaxQ_Action_Count:98, Epsilon:0.53144\n",
      "Episode:6, Avg_Score:-232.4, MaxQ_Action_Count:98, Epsilon:0.47830\n",
      "Episode:7, Avg_Score:-487.8, MaxQ_Action_Count:112, Epsilon:0.43047\n",
      "Episode:8, Avg_Score:-380.0, MaxQ_Action_Count:109, Epsilon:0.38742\n",
      "Episode:9, Avg_Score:-376.7, MaxQ_Action_Count:139, Epsilon:0.34868\n",
      "Episode:10, Avg_Score:-380.6, MaxQ_Action_Count:132, Epsilon:0.31381\n",
      "Episode:11, Avg_Score:-749.9, MaxQ_Action_Count:137, Epsilon:0.28243\n",
      "Episode:12, Avg_Score:-373.6, MaxQ_Action_Count:154, Epsilon:0.25419\n",
      "Episode:13, Avg_Score:-395.5, MaxQ_Action_Count:147, Epsilon:0.22877\n",
      "Episode:14, Avg_Score:-127.0, MaxQ_Action_Count:156, Epsilon:0.20589\n",
      "Episode:15, Avg_Score:-255.0, MaxQ_Action_Count:156, Epsilon:0.18530\n",
      "Episode:16, Avg_Score:-375.8, MaxQ_Action_Count:170, Epsilon:0.16677\n",
      "Episode:17, Avg_Score:-130.1, MaxQ_Action_Count:168, Epsilon:0.15009\n",
      "Episode:18, Avg_Score:-403.9, MaxQ_Action_Count:176, Epsilon:0.13509\n",
      "Episode:19, Avg_Score:-132.0, MaxQ_Action_Count:176, Epsilon:0.12158\n",
      "Episode:20, Avg_Score:-251.1, MaxQ_Action_Count:178, Epsilon:0.10942\n",
      "Episode:21, Avg_Score:-125.8, MaxQ_Action_Count:179, Epsilon:0.09848\n",
      "Episode:22, Avg_Score:-381.5, MaxQ_Action_Count:183, Epsilon:0.08863\n",
      "Episode:23, Avg_Score:-238.0, MaxQ_Action_Count:184, Epsilon:0.07977\n",
      "Episode:24, Avg_Score:-3.2, MaxQ_Action_Count:184, Epsilon:0.07179\n",
      "Episode:25, Avg_Score:-229.5, MaxQ_Action_Count:195, Epsilon:0.06461\n",
      "Episode:26, Avg_Score:-236.1, MaxQ_Action_Count:179, Epsilon:0.05815\n",
      "Episode:27, Avg_Score:-357.1, MaxQ_Action_Count:191, Epsilon:0.05233\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 160\u001b[0m\n\u001b[0;32m    156\u001b[0m state_prime, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep([real_action])\n\u001b[0;32m    158\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mput((state, action, reward, state_prime, terminated))\n\u001b[1;32m--> 160\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[0;32m    163\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 113\u001b[0m, in \u001b[0;36mDQNAgent.train_agent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m mini_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m    112\u001b[0m s_batch, a_batch, r_batch, s_prime_batch, done_batch \u001b[38;5;241m=\u001b[39m mini_batch\n\u001b[1;32m--> 113\u001b[0m a_batch \u001b[38;5;241m=\u001b[39m \u001b[43ma_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m td_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_target(mini_batch)\n\u001b[0;32m    117\u001b[0m Q_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ(s_batch)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, a_batch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the action transformation function\n",
    "def transform_action_values(n_actions, min_action=-2.0, max_action=2.0):\n",
    "    linear_actions = np.linspace(-1, 1, n_actions)\n",
    "    non_linear_actions = np.sign(linear_actions) * (linear_actions ** 2)\n",
    "    scaled_actions = min_action + (non_linear_actions + 1) * (max_action - min_action) / 2\n",
    "    return scaled_actions\n",
    "\n",
    "# Define the replay buffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        s_batch = torch.tensor(s_lst, dtype=torch.float)\n",
    "        a_batch = torch.tensor(a_lst, dtype=torch.float)\n",
    "        r_batch = torch.tensor(r_lst, dtype=torch.float)\n",
    "        s_prime_batch = torch.tensor(s_prime_lst, dtype=torch.float)\n",
    "        done_batch = torch.tensor(done_mask_lst, dtype=torch.float)\n",
    "\n",
    "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Define the Q-network class\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, q_lr):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc_1 = nn.Linear(state_dim, 64)\n",
    "        self.fc_2 = nn.Linear(64, 32)\n",
    "        self.fc_out = nn.Linear(32, action_dim)\n",
    "        self.lr = q_lr\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = F.leaky_relu(self.fc_1(x))\n",
    "        q = F.leaky_relu(self.fc_2(q))\n",
    "        q = self.fc_out(q)\n",
    "        return q\n",
    "\n",
    "# Define the DQN agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.state_dim = 3\n",
    "        self.action_dim = 11\n",
    "        self.lr = 0.01\n",
    "        self.gamma = 0.995\n",
    "        self.tau = 0.01\n",
    "        self.epsilon = 0.9\n",
    "        self.epsilon_decay = 0.9\n",
    "        self.epsilon_min = 0.001\n",
    "        self.buffer_size = 1000000\n",
    "        self.batch_size = 256\n",
    "        self.memory = ReplayBuffer(self.buffer_size)\n",
    "        self.action_list = transform_action_values(self.action_dim, min_action=-2.0, max_action=2.0)\n",
    "\n",
    "        self.Q = QNetwork(self.state_dim, self.action_dim, self.lr)\n",
    "        self.Q_target = QNetwork(self.state_dim, self.action_dim, self.lr)\n",
    "        self.Q_target.load_state_dict(self.Q.state_dict())\n",
    "\n",
    "    def select_action(self, state):\n",
    "        random_number = np.random.rand()\n",
    "        maxQ_action_count = 0\n",
    "        if self.epsilon < random_number:\n",
    "            with torch.no_grad():\n",
    "                action = float(torch.argmax(self.Q(state)).numpy())\n",
    "                maxQ_action_count = 1\n",
    "        else:\n",
    "            action = float(np.random.choice([n for n in range(self.action_dim)]))\n",
    "        real_action = self.action_list[int(action)]\n",
    "\n",
    "        return action, real_action, maxQ_action_count\n",
    "\n",
    "    def calc_target(self, mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        with torch.no_grad():\n",
    "            q_target = self.Q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "            target = r + self.gamma * done * q_target\n",
    "        return target\n",
    "\n",
    "    def train_agent(self):\n",
    "        mini_batch = self.memory.sample(self.batch_size)\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = mini_batch\n",
    "        a_batch = a_batch.type(torch.int64)\n",
    "\n",
    "        td_target = self.calc_target(mini_batch)\n",
    "\n",
    "        Q_a = self.Q(s_batch).gather(1, a_batch)\n",
    "        q_loss = F.smooth_l1_loss(Q_a, td_target)\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        q_loss.mean().backward()\n",
    "        self.Q.optimizer.step()\n",
    "\n",
    "        # Q soft-update\n",
    "        for param_target, param in zip(self.Q_target.parameters(), self.Q.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    agent = DQNAgent()\n",
    "    env = gym.make('Pendulum-v1')\n",
    "\n",
    "    EPOCHS = 100\n",
    "    score_list = []\n",
    "\n",
    "    while agent.memory.size() < 4 * agent.batch_size:\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, real_action, _ = agent.select_action(torch.FloatTensor(state))\n",
    "            state_prime, reward, terminated, truncated, _ = env.step([real_action])\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "            agent.memory.put((state, action, reward, state_prime, terminated))\n",
    "            state = state_prime\n",
    "\n",
    "    for EP in range(EPOCHS):\n",
    "        state, info = env.reset()\n",
    "        score, done = 0.0, False\n",
    "        maxQ_action_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action, real_action, count = agent.select_action(torch.FloatTensor(state))\n",
    "\n",
    "            state_prime, reward, terminated, truncated, _ = env.step([real_action])\n",
    "\n",
    "            agent.memory.put((state, action, reward, state_prime, terminated))\n",
    "\n",
    "            agent.train_agent()\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "            score += reward\n",
    "            maxQ_action_count += count\n",
    "\n",
    "            state = state_prime\n",
    "\n",
    "        print(\"Epoch:{}, Avg_Score:{:.1f}, MaxQ_Action_Count:{}, Epsilon:{:.5f}\".format(EP, score, maxQ_action_count, agent.epsilon))\n",
    "        score_list.append(score)\n",
    "        agent.epsilon = max(agent.epsilon_min, agent.epsilon*agent.epsilon_decay)\n",
    "\n",
    "    plt.plot(score_list)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
