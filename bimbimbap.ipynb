{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zachu\\AppData\\Local\\Temp\\ipykernel_16732\\1610629461.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  s_batch = torch.tensor(s_lst, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP:0, Avg_Score:-1757.9, MaxQ_Action_Count:25, Epsilon:0.90000\n",
      "EP:1, Avg_Score:-1384.1, MaxQ_Action_Count:33, Epsilon:0.81000\n",
      "EP:2, Avg_Score:-1407.9, MaxQ_Action_Count:50, Epsilon:0.72900\n",
      "EP:3, Avg_Score:-1505.3, MaxQ_Action_Count:71, Epsilon:0.65610\n",
      "EP:4, Avg_Score:-1074.9, MaxQ_Action_Count:87, Epsilon:0.59049\n",
      "EP:5, Avg_Score:-635.4, MaxQ_Action_Count:98, Epsilon:0.53144\n",
      "EP:6, Avg_Score:-971.7, MaxQ_Action_Count:88, Epsilon:0.47830\n",
      "EP:7, Avg_Score:-745.1, MaxQ_Action_Count:116, Epsilon:0.43047\n",
      "EP:8, Avg_Score:-3.7, MaxQ_Action_Count:125, Epsilon:0.38742\n",
      "EP:9, Avg_Score:-340.7, MaxQ_Action_Count:137, Epsilon:0.34868\n",
      "EP:10, Avg_Score:-353.7, MaxQ_Action_Count:139, Epsilon:0.31381\n",
      "EP:11, Avg_Score:-1371.8, MaxQ_Action_Count:135, Epsilon:0.28243\n",
      "EP:12, Avg_Score:-128.5, MaxQ_Action_Count:150, Epsilon:0.25419\n",
      "EP:13, Avg_Score:-2.4, MaxQ_Action_Count:152, Epsilon:0.22877\n",
      "EP:14, Avg_Score:-2.4, MaxQ_Action_Count:163, Epsilon:0.20589\n",
      "EP:15, Avg_Score:-1218.5, MaxQ_Action_Count:159, Epsilon:0.18530\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 172\u001b[0m\n\u001b[0;32m    168\u001b[0m state_prime, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep([real_action])\n\u001b[0;32m    170\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mput((state, action, reward, state_prime, terminated))\n\u001b[1;32m--> 172\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[0;32m    175\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 120\u001b[0m, in \u001b[0;36mDQNAgent.train_agent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m mini_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m    119\u001b[0m s_batch, a_batch, r_batch, s_prime_batch, done_batch \u001b[38;5;241m=\u001b[39m mini_batch\n\u001b[1;32m--> 120\u001b[0m a_batch \u001b[38;5;241m=\u001b[39m \u001b[43ma_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m td_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_target(mini_batch)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m#### Q train ####\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def transform_action_values(n_actions, min_action=-2.0, max_action=2.0):\n",
    "    # Linearly spaced values between -1 and 1\n",
    "    linear_actions = np.linspace(-1, 1, n_actions)\n",
    "\n",
    "    # Apply a quadratic transformation\n",
    "    non_linear_actions = np.sign(linear_actions) * (linear_actions ** 2)\n",
    "\n",
    "    # Scale to the action range of the environment\n",
    "    scaled_actions = min_action + (non_linear_actions + 1) * (max_action - min_action) / 2\n",
    "    return scaled_actions\n",
    "\n",
    "# ReplayBuffer from https://github.com/seungeunrho/minimalRL\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        s_batch = torch.tensor(s_lst, dtype=torch.float)\n",
    "        a_batch = torch.tensor(a_lst, dtype=torch.float)\n",
    "        r_batch = torch.tensor(r_lst, dtype=torch.float)\n",
    "        s_prime_batch = torch.tensor(s_prime_lst, dtype=torch.float)\n",
    "        done_batch = torch.tensor(done_mask_lst, dtype=torch.float)\n",
    "\n",
    "        # r_batch = (r_batch - r_batch.mean()) / (r_batch.std() + 1e-7)\n",
    "\n",
    "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, q_lr):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(state_dim, 64)\n",
    "        self.fc_2 = nn.Linear(64, 32)\n",
    "        self.fc_out = nn.Linear(32, action_dim)\n",
    "\n",
    "        self.lr = q_lr\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = F.leaky_relu(self.fc_1(x))\n",
    "        q = F.leaky_relu(self.fc_2(q))\n",
    "        q = self.fc_out(q)\n",
    "        return q\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.state_dim     = 3\n",
    "        self.action_dim    = 11\n",
    "        self.lr            = 0.01\n",
    "        self.gamma         = 0.995\n",
    "        self.tau           = 0.01\n",
    "        self.epsilon       = 0.9\n",
    "        self.epsilon_decay = 0.9\n",
    "        self.epsilon_min   = 0.001\n",
    "        self.buffer_size   = 1000000\n",
    "        self.batch_size    = 256\n",
    "        self.memory        = ReplayBuffer(self.buffer_size)\n",
    "        self.action_list = transform_action_values(self.action_dim, min_action=-2.0, max_action=2.0)\n",
    "\n",
    "        self.Q        = QNetwork(self.state_dim, self.action_dim, self.lr)\n",
    "        self.Q_target = QNetwork(self.state_dim, self.action_dim, self.lr)\n",
    "        self.Q_target.load_state_dict(self.Q.state_dict())\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        random_number = np.random.rand()\n",
    "        maxQ_action_count = 0\n",
    "        if self.epsilon < random_number:\n",
    "            with torch.no_grad():\n",
    "                action = float(torch.argmax(self.Q(state)).numpy())\n",
    "                maxQ_action_count = 1\n",
    "        else:\n",
    "            action = float(np.random.choice([n for n in range(self.action_dim)]))\n",
    "        real_action = self.action_list[int(action)]\n",
    "\n",
    "        return action, real_action, maxQ_action_count\n",
    "\n",
    "    def calc_target(self, mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        with torch.no_grad():\n",
    "            q_target = self.Q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "            target = r + self.gamma * done * q_target\n",
    "        return target\n",
    "\n",
    "    def train_agent(self):\n",
    "        mini_batch = self.memory.sample(self.batch_size)\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = mini_batch\n",
    "        a_batch = a_batch.type(torch.int64)\n",
    "\n",
    "        td_target = self.calc_target(mini_batch)\n",
    "\n",
    "        #### Q train ####\n",
    "        Q_a = self.Q(s_batch).gather(1, a_batch)\n",
    "        q_loss = F.smooth_l1_loss(Q_a, td_target)\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        q_loss.mean().backward()\n",
    "        self.Q.optimizer.step()\n",
    "        #### Q train ####\n",
    "\n",
    "        #### Q soft-update ####\n",
    "        for param_target, param in zip(self.Q_target.parameters(), self.Q.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNAgent()\n",
    "    env = gym.make('Pendulum-v1')\n",
    "\n",
    "    EPISODE = 50\n",
    "    print_once = True\n",
    "    score_list = []\n",
    "\n",
    "    # Fill up the replay buffer with random actions\n",
    "    while agent.memory.size() < 4 * agent.batch_size:\n",
    "        state, _info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, real_action, _ = agent.choose_action(torch.FloatTensor(state))\n",
    "            state_prime, reward, terminated, truncated, _ = env.step([real_action])\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "            agent.memory.put((state, action, reward, state_prime, terminated))\n",
    "            state = state_prime\n",
    "\n",
    "    for EP in range(EPISODE):\n",
    "        state, _info = env.reset()\n",
    "        score, done = 0.0, False\n",
    "        maxQ_action_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action, real_action, count = agent.choose_action(torch.FloatTensor(state))\n",
    "\n",
    "            state_prime, reward, terminated, truncated, _ = env.step([real_action])\n",
    "\n",
    "            agent.memory.put((state, action, reward, state_prime, terminated))\n",
    "\n",
    "            agent.train_agent()\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "            score += reward\n",
    "            maxQ_action_count += count\n",
    "\n",
    "            state = state_prime\n",
    "\n",
    "        print(\"EP:{}, Avg_Score:{:.1f}, MaxQ_Action_Count:{}, Epsilon:{:.5f}\".format(EP, score, maxQ_action_count, agent.epsilon))\n",
    "        score_list.append(score)\n",
    "        agent.epsilon = max(agent.epsilon_min, agent.epsilon*agent.epsilon_decay)\n",
    "\n",
    "    # score = [float(s) for s in data]\n",
    "\n",
    "    plt.plot(score_list)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials(agent, num_trials=10):\n",
    "    env = gym.make('Pendulum-v1', render_mode='human')\n",
    "    for trial in range(num_trials):\n",
    "        state, _info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, real_action, count = agent.choose_action(torch.FloatTensor(state))\n",
    "            state_prime, reward, terminated, truncated, _ = env.step([real_action])\n",
    "\n",
    "            state = state_prime\n",
    "        print(f\"Trial {trial + 1} completed\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [37], line 8\u001b[0m, in \u001b[0;36mrun_trials\u001b[1;34m(agent, num_trials)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      7\u001b[0m     action, real_action, count \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(torch\u001b[38;5;241m.\u001b[39mFloatTensor(state))\n\u001b[1;32m----> 8\u001b[0m     state_prime, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreal_action\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     state \u001b[38;5;241m=\u001b[39m state_prime\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\torch_env\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\torch_env\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\torch_env\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\torch_env\\lib\\site-packages\\gymnasium\\envs\\classic_control\\pendulum.py:143\u001b[0m, in \u001b[0;36mPendulumEnv.step\u001b[1;34m(self, u)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([newth, newthdot])\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs(), \u001b[38;5;241m-\u001b[39mcosts, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\zachary\\anaconda3\\envs\\torch_env\\lib\\site-packages\\gymnasium\\envs\\classic_control\\pendulum.py:259\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    258\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 259\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# mode == \"rgb_array\":\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_trials(agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
