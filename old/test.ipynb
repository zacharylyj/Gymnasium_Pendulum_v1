{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zachu\\.conda\\envs\\tensorflow_env1\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "c:\\Users\\zachu\\.conda\\envs\\tensorflow_env1\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "c:\\Users\\zachu\\.conda\\envs\\tensorflow_env1\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\zachu\\.conda\\envs\\tensorflow_env1\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:252: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zachu\\AppData\\Local\\Temp\\ipykernel_12796\\2987097267.py:62: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  target_f[0][0] = target\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[0;32m     80\u001b[0m     batch \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[1;32m---> 81\u001b[0m     \u001b[43mupdate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m, in \u001b[0;36mupdate_model\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     60\u001b[0m     target \u001b[38;5;241m=\u001b[39m (reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(target_model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray([next_state]))))\n\u001b[1;32m---> 61\u001b[0m target_f \u001b[38;5;241m=\u001b[39m dqn_model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     62\u001b[0m target_f[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m target\n\u001b[0;32m     63\u001b[0m dqn_model\u001b[38;5;241m.\u001b[39mfit(np\u001b[38;5;241m.\u001b[39marray([state]), target_f, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "def build_model(input_shape, action_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(action_size, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Set up the environment\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "num_actions = 10\n",
    "action_bins = np.linspace(-2.0, 2.0, num_actions)\n",
    "\n",
    "def build_model(input_shape, action_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(action_size, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create the DQN model\n",
    "state_shape = env.observation_space.shape\n",
    "model = build_model(state_shape, num_actions)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "gamma = 0.95  # Discount factor for past rewards\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "# Function to update the Q-values\n",
    "def update_q_values(minibatch):\n",
    "    states = np.vstack([transition[0] for transition in minibatch])\n",
    "    actions = np.array([transition[1] for transition in minibatch])\n",
    "    rewards = np.array([transition[2] for transition in minibatch])\n",
    "    next_states = np.vstack([transition[3] for transition in minibatch])\n",
    "    dones = np.array([transition[4] for transition in minibatch])\n",
    "\n",
    "    q_values = model.predict(states)\n",
    "    q_values_next = model.predict(next_states)\n",
    "\n",
    "    for i in range(len(minibatch)):\n",
    "        if dones[i]:\n",
    "            q_values[i][actions[i]] = rewards[i]\n",
    "        else:\n",
    "            q_values[i][actions[i]] = rewards[i] + gamma * np.max(q_values_next[i])\n",
    "    return states, q_values\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "memory = deque(maxlen=10000)\n",
    "\n",
    "# Function to choose an action\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    q_values = model.predict(state.reshape(1, -1))\n",
    "    return np.argmax(q_values[0])\n",
    "\n",
    "# Function to discretize the action for the environment\n",
    "def get_continuous_action(discrete_action):\n",
    "    return [action_bins[discrete_action]]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0 \n",
    "\n",
    "    while not done:\n",
    "        action = choose_action(state, epsilon=1.0/(epoch+1))\n",
    "        continuous_action = get_continuous_action(action)\n",
    "        print(env.step(continuous_action))\n",
    "        next_state, reward, done, _, _ = env.step(continuous_action)\n",
    "\n",
    "        # Handle state whether it's a tuple or directly an array\n",
    "        state_array = state[0] if isinstance(state, tuple) else state\n",
    "        next_state_array = next_state[0] if isinstance(next_state, tuple) else next_state\n",
    "        memory.append((state_array, action, reward, next_state_array, done))\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        # Train the model if memory is sufficient\n",
    "        if len(memory) > batch_size:\n",
    "            minibatch = random.sample(memory, batch_size)\n",
    "            x_train, y_train = update_q_values(minibatch)\n",
    "            model.train_on_batch(x_train, y_train)\n",
    "\n",
    "    rewards_history.append(total_reward)\n",
    "    print(f\"Epoch: {epoch}, Total Reward: {total_reward}\")\n",
    "\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        plot_rewards(rewards_history, epoch + 1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
